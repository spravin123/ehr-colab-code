{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spravin123/ehr-colab-code/blob/main/transfer_(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gfKU2vvzLsz",
        "outputId": "aaae454f-9a0e-480b-b35b-9f4104c90839"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gcsfs in /usr/local/lib/python3.11/dist-packages (2025.3.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from gcsfs) (3.11.15)\n",
            "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.11/dist-packages (from gcsfs) (4.4.2)\n",
            "Requirement already satisfied: fsspec==2025.3.2 in /usr/local/lib/python3.11/dist-packages (from gcsfs) (2025.3.2)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.11/dist-packages (from gcsfs) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.11/dist-packages (from gcsfs) (1.2.1)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.11/dist-packages (from gcsfs) (2.19.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from gcsfs) (2.32.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.19.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib->gcsfs) (2.0.0)\n",
            "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs) (2.24.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs) (2.7.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs) (1.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->gcsfs) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->gcsfs) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->gcsfs) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->gcsfs) (2025.1.31)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs) (1.70.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs) (5.29.4)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs) (1.26.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install gcsfs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "EHvFe7hSzdxE"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jla_Z7yiz5mB",
        "outputId": "f9a78b6c-91cf-4a3f-d7e1-35be985fc86d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://ehr-project/ClinicalBERT_checkpoint/ClinicalBERT_discharge_pytorch_checkpoint/pytorch_model.bin...\n",
            "/ [0/9 files][    0.0 B/  1.2 GiB]   0% Done                                    \rCopying gs://ehr-project/ClinicalBERT_checkpoint/ClinicalBERT_discharge_pytorch_checkpoint/.DS_Store...\n",
            "==> NOTE: You are downloading one or more large file(s), which would\n",
            "run significantly faster if you enabled sliced object downloads. This\n",
            "feature is enabled by default but requires that compiled crcmod be\n",
            "installed (see \"gsutil help crcmod\").\n",
            "\n",
            "/ [0/9 files][    0.0 B/  1.2 GiB]   0% Done                                    \rCopying gs://ehr-project/ClinicalBERT_checkpoint/ClinicalBERT_discharge_pytorch_checkpoint/bert_config.json...\n",
            "/ [0/9 files][    0.0 B/  1.2 GiB]   0% Done                                    \rCopying gs://ehr-project/ClinicalBERT_checkpoint/ClinicalBERT_discharge_pytorch_checkpoint/vocab.txt...\n",
            "/ [0/9 files][    0.0 B/  1.2 GiB]   0% Done                                    \rCopying gs://ehr-project/ClinicalBERT_checkpoint/ClinicalBERT_early_notes_pytorch_checkpoint/bert_config.json...\n",
            "/ [0/9 files][    0.0 B/  1.2 GiB]   0% Done                                    \rCopying gs://ehr-project/ClinicalBERT_checkpoint/ClinicalBERT_pretraining_pytorch_checkpoint/bert_config.json...\n",
            "Copying gs://ehr-project/ClinicalBERT_checkpoint/.DS_Store...\n",
            "/ [0/9 files][    0.0 B/  1.2 GiB]   0% Done                                    \r/ [0/9 files][    0.0 B/  1.2 GiB]   0% Done                                    \rCopying gs://ehr-project/ClinicalBERT_checkpoint/ClinicalBERT_early_notes_pytorch_checkpoint/pytorch_model.bin...\n",
            "Copying gs://ehr-project/ClinicalBERT_checkpoint/ClinicalBERT_pretraining_pytorch_checkpoint/pytorch_model.bin...\n",
            "- [9/9 files][  1.2 GiB/  1.2 GiB] 100% Done 114.1 MiB/s ETA 00:00:00           \n",
            "Operation completed over 9 objects/1.2 GiB.                                      \n"
          ]
        }
      ],
      "source": [
        "!gsutil -m cp -r gs://ehr-project/ClinicalBERT_checkpoint /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvFsvgG_0O7z",
        "outputId": "b4edad83-46e1-4070-fa82-bc7ba87f8f9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://ehr-project/pytt/.git/FETCH_HEAD...\n",
            "Copying gs://ehr-project/pytt/.git/HEAD...\n",
            "/ [0/91 files][    0.0 B/490.8 KiB]   0% Done                                   \r/ [0/91 files][    0.0 B/490.8 KiB]   0% Done                                   \rCopying gs://ehr-project/pytt/.git/config...\n",
            "/ [0/91 files][    0.0 B/490.8 KiB]   0% Done                                   \rCopying gs://ehr-project/pytt/.git/description...\n",
            "Copying gs://ehr-project/pytt/.git/hooks/applypatch-msg.sample...\n",
            "/ [0/91 files][    0.0 B/490.8 KiB]   0% Done                                   \r/ [0/91 files][    0.0 B/490.8 KiB]   0% Done                                   \rCopying gs://ehr-project/pytt/.git/hooks/post-update.sample...\n",
            "/ [0/91 files][    0.0 B/490.8 KiB]   0% Done                                   \rCopying gs://ehr-project/pytt/.git/hooks/fsmonitor-watchman.sample...\n",
            "/ [0/91 files][    0.0 B/490.8 KiB]   0% Done                                   \rCopying gs://ehr-project/pytt/.git/hooks/commit-msg.sample...\n",
            "/ [0/91 files][    0.0 B/490.8 KiB]   0% Done                                   \rCopying gs://ehr-project/pytt/.git/hooks/pre-applypatch.sample...\n",
            "Copying gs://ehr-project/pytt/.git/hooks/pre-commit.sample...\n",
            "Copying gs://ehr-project/pytt/.git/hooks/pre-merge-commit.sample...\n",
            "Copying gs://ehr-project/pytt/.git/hooks/pre-push.sample...\n",
            "Copying gs://ehr-project/pytt/.git/hooks/pre-rebase.sample...\n",
            "Copying gs://ehr-project/pytt/.git/hooks/pre-receive.sample...\n",
            "Copying gs://ehr-project/pytt/.git/hooks/prepare-commit-msg.sample...\n",
            "Copying gs://ehr-project/pytt/.git/hooks/push-to-checkout.sample...\n",
            "Copying gs://ehr-project/pytt/.git/hooks/sendemail-validate.sample...\n",
            "Copying gs://ehr-project/pytt/.git/hooks/update.sample...\n",
            "Copying gs://ehr-project/pytt/.git/index...\n",
            "Copying gs://ehr-project/pytt/.git/info/exclude...\n",
            "Copying gs://ehr-project/pytt/.git/logs/HEAD...\n",
            "Copying gs://ehr-project/pytt/.git/logs/refs/heads/master...\n",
            "Copying gs://ehr-project/pytt/.git/logs/refs/remotes/origin/HEAD...\n",
            "Copying gs://ehr-project/pytt/.git/objects/pack/pack-12721ed6cbccd6e80ec343633f3077cebcd32ddd.idx...\n",
            "Copying gs://ehr-project/pytt/.git/objects/pack/pack-12721ed6cbccd6e80ec343633f3077cebcd32ddd.pack...\n",
            "Copying gs://ehr-project/pytt/.git/objects/pack/pack-12721ed6cbccd6e80ec343633f3077cebcd32ddd.rev...\n",
            "Copying gs://ehr-project/pytt/.git/packed-refs...\n",
            "Copying gs://ehr-project/pytt/.git/refs/heads/master...\n",
            "Copying gs://ehr-project/pytt/.git/refs/remotes/origin/HEAD...\n",
            "Copying gs://ehr-project/pytt/.gitignore...\n",
            "Copying gs://ehr-project/pytt/README.md...\n",
            "Copying gs://ehr-project/pytt/pytt.egg-info/PKG-INFO...\n",
            "Copying gs://ehr-project/pytt/pytt.egg-info/SOURCES.txt...\n",
            "Copying gs://ehr-project/pytt/pytt.egg-info/dependency_links.txt...\n",
            "Copying gs://ehr-project/pytt/pytt.egg-info/top_level.txt...\n",
            "Copying gs://ehr-project/pytt/pytt/__pycache__/distributed.cpython-310.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/__pycache__/distributed.cpython-312.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/__pycache__/email.cpython-310.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/__pycache__/email.cpython-312.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/__pycache__/logger.cpython-310.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/__pycache__/logger.cpython-312.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/__pycache__/progress_bar.cpython-310.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/__pycache__/progress_bar.cpython-312.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/__pycache__/utils.cpython-310.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/__pycache__/utils.cpython-312.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/batching/__pycache__/abstract_batcher.cpython-310.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/batching/__pycache__/abstract_batcher.cpython-312.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/batching/__pycache__/indices_iterator.cpython-310.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/batching/__pycache__/indices_iterator.cpython-312.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/batching/__pycache__/postprocessor.cpython-310.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/batching/__pycache__/postprocessor.cpython-312.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/batching/__pycache__/standard_batch_iterator.cpython-310.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/batching/__pycache__/standard_batch_iterator.cpython-312.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/batching/__pycache__/standard_batcher.cpython-310.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/batching/__pycache__/standard_batcher.cpython-312.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/batching/abstract_batcher.py...\n",
            "Copying gs://ehr-project/pytt/pytt/batching/indices_iterator.py...\n",
            "Copying gs://ehr-project/pytt/pytt/batching/postprocessor.py...\n",
            "Copying gs://ehr-project/pytt/pytt/batching/standard_batch_iterator.py...\n",
            "Copying gs://ehr-project/pytt/pytt/batching/standard_batcher.py...\n",
            "Copying gs://ehr-project/pytt/pytt/distributed.py...\n",
            "Copying gs://ehr-project/pytt/pytt/email.py...\n",
            "Copying gs://ehr-project/pytt/pytt/logger.py...\n",
            "Copying gs://ehr-project/pytt/pytt/nlp/tokenizer.py...\n",
            "Copying gs://ehr-project/pytt/pytt/preprocessing/__pycache__/raw_dataset.cpython-310.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/preprocessing/__pycache__/raw_dataset.cpython-312.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/preprocessing/raw_dataset.py...\n",
            "Copying gs://ehr-project/pytt/pytt/progress_bar.py...\n",
            "Copying gs://ehr-project/pytt/pytt/testing/__pycache__/datapoint_processor.cpython-310.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/testing/__pycache__/datapoint_processor.cpython-312.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/testing/__pycache__/tester.cpython-310.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/testing/__pycache__/tester.cpython-312.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/testing/datapoint_processor.py...\n",
            "Copying gs://ehr-project/pytt/pytt/testing/raw_individual_processor.py...\n",
            "Copying gs://ehr-project/pytt/pytt/testing/tester.py...\n",
            "Copying gs://ehr-project/pytt/pytt/training/__pycache__/iteration_info.cpython-310.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/training/__pycache__/iteration_info.cpython-312.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/training/__pycache__/tracker.cpython-310.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/training/__pycache__/tracker.cpython-312.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/training/__pycache__/trainer.cpython-310.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/training/__pycache__/trainer.cpython-312.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/training/iteration_info.py...\n",
            "Copying gs://ehr-project/pytt/pytt/training/tracker.py...\n",
            "Copying gs://ehr-project/pytt/pytt/training/trainer.py...\n",
            "Copying gs://ehr-project/pytt/pytt/training/training_controller.py...\n",
            "Copying gs://ehr-project/pytt/pytt/utils.py...\n",
            "Copying gs://ehr-project/pytt/requirements.txt...\n",
            "Copying gs://ehr-project/pytt/setup.py...\n",
            "Copying gs://ehr-project/pytt/summarization/summarization_batcher.py...\n",
            "Copying gs://ehr-project/pytt/summarization/summarization_dataset.py...\n",
            "Copying gs://ehr-project/pytt/tests.py...\n",
            "- [91/91 files][490.8 KiB/490.8 KiB] 100% Done                                  \n",
            "Operation completed over 91 objects/490.8 KiB.                                   \n"
          ]
        }
      ],
      "source": [
        "!gsutil -m cp -r gs://ehr-project/pytt /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8k4fyFX0d16",
        "outputId": "3ed277d8-f84a-4dfe-a642-e4823370d9bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://ehr-project/mimic_synthetic_data_100-assembled/counts.pkl...\n",
            "Copying gs://ehr-project/mimic_synthetic_data_100-assembled/test.data...\n",
            "Copying gs://ehr-project/mimic_synthetic_data_100-assembled/train.data...\n",
            "Copying gs://ehr-project/mimic_synthetic_data_100-assembled/used_targets.txt...\n",
            "- [4 files][  9.8 MiB/  9.8 MiB]                                                \n",
            "==> NOTE: You are performing a sequence of gsutil operations that may\n",
            "run significantly faster if you instead use gsutil -m cp ... Please\n",
            "see the -m section under \"gsutil help options\" for further information\n",
            "about when gsutil -m can be advantageous.\n",
            "\n",
            "Copying gs://ehr-project/mimic_synthetic_data_100-assembled/val.data...\n",
            "- [5 files][ 11.6 MiB/ 11.6 MiB]                                                \n",
            "Operation completed over 5 objects/11.6 MiB.                                     \n"
          ]
        }
      ],
      "source": [
        "!gsutil cp -r gs://ehr-project/mimic_synthetic_data_100-assembled /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAppaZDq0mHZ",
        "outputId": "54a42a0f-dc4c-415f-c1e6-d1b370b3d885"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://ehr-project/ehr-extraction-models/.DS_Store...\n",
            "Copying gs://ehr-project/ehr-extraction-models/.git/.DS_Store...\n",
            "Copying gs://ehr-project/ehr-extraction-models/.git/COMMIT_EDITMSG...\n",
            "Copying gs://ehr-project/ehr-extraction-models/.git/FETCH_HEAD...\n",
            "- [4 files][ 17.4 KiB/ 17.4 KiB]                                                \n",
            "==> NOTE: You are performing a sequence of gsutil operations that may\n",
            "run significantly faster if you instead use gsutil -m cp ... Please\n",
            "see the -m section under \"gsutil help options\" for further information\n",
            "about when gsutil -m can be advantageous.\n",
            "\n",
            "Copying gs://ehr-project/ehr-extraction-models/.git/HEAD...\n",
            "Copying gs://ehr-project/ehr-extraction-models/.git/config...\n",
            "Copying gs://ehr-project/ehr-extraction-models/.git/description...\n",
            "Copying gs://ehr-project/ehr-extraction-models/.git/hooks/applypatch-msg.sample...\n",
            "Copying gs://ehr-project/ehr-extraction-models/.git/hooks/commit-msg.sample...\n",
            "Copying gs://ehr-project/ehr-extraction-models/.git/hooks/fsmonitor-watchman.sample...\n",
            "Copying gs://ehr-project/ehr-extraction-models/.git/hooks/post-update.sample...\n",
            "Copying gs://ehr-project/ehr-extraction-models/.git/hooks/pre-applypatch.sample...\n",
            "Copying gs://ehr-project/ehr-extraction-models/.git/hooks/pre-commit.sample...\n",
            "Copying gs://ehr-project/ehr-extraction-models/.git/hooks/pre-merge-commit.sample...\n",
            "Copying gs://ehr-project/ehr-extraction-models/.git/hooks/pre-push.sample...\n",
            "Copying gs://ehr-project/ehr-extraction-models/.git/hooks/pre-rebase.sample...\n",
            "Copying gs://ehr-project/ehr-extraction-models/.git/hooks/pre-receive.sample...\n",
            "Copying gs://ehr-project/ehr-extraction-models/.git/hooks/prepare-commit-msg.sample...\n",
            "Copying gs://ehr-project/ehr-extraction-models/.git/hooks/push-to-checkout.sample...\n",
            "Copying gs://ehr-project/ehr-extraction-models/.git/hooks/sendemail-validate.sample...\n",
            "Copying gs://ehr-project/ehr-extraction-models/.git/hooks/update.sample...\n",
            "Copying gs://ehr-project/ehr-extraction-models/.git/index...\n",
            "Copying gs://ehr-project/ehr-extraction-models/.git/info/exclude...\n",
            "Copying gs://ehr-project/ehr-extraction-models/.git/logs/HEAD...\n",
            "Copying gs://ehr-project/ehr-extraction-models/.git/logs/refs/heads/master...\n",
            "Copying gs://ehr-project/ehr-extraction-models/.git/logs/refs/remotes/origin/HEAD...\n",
            "Copying gs://ehr-project/ehr-extraction-models/.git/objects/73/254b255f88e365dc4af8d3dd0e1ed9a5f68ded...\n",
            "Copying gs://ehr-project/ehr-extraction-models/.git/objects/80/a9022e5a1091578a1aba69fb149b8971515f43...\n",
            "Copying gs://ehr-project/ehr-extraction-models/.git/objects/ad/953017fbb41dd290a8a3a634c0c49d51ba1811...\n",
            "Copying gs://ehr-project/ehr-extraction-models/.git/objects/pack/pack-c2b5417178549a0fb2ef943b36786bf70273dde0.idx...\n",
            "Copying gs://ehr-project/ehr-extraction-models/.git/objects/pack/pack-c2b5417178549a0fb2ef943b36786bf70273dde0.pack...\n",
            "Copying gs://ehr-project/ehr-extraction-models/.git/objects/pack/pack-c2b5417178549a0fb2ef943b36786bf70273dde0.rev...\n",
            "Copying gs://ehr-project/ehr-extraction-models/.git/packed-refs...\n",
            "Copying gs://ehr-project/ehr-extraction-models/.git/refs/heads/master...\n",
            "Copying gs://ehr-project/ehr-extraction-models/.git/refs/remotes/origin/HEAD...\n",
            "Copying gs://ehr-project/ehr-extraction-models/.gitignore...\n",
            "Copying gs://ehr-project/ehr-extraction-models/.vscode/launch.json...\n",
            "Copying gs://ehr-project/ehr-extraction-models/LICENSE...\n",
            "Copying gs://ehr-project/ehr-extraction-models/README.md...\n",
            "Copying gs://ehr-project/ehr-extraction-models/calculate_random_best.py...\n",
            "Copying gs://ehr-project/ehr-extraction-models/context_vector_analysis.ipynb...\n",
            "Copying gs://ehr-project/ehr-extraction-models/convert_model_weights.py...\n",
            "Copying gs://ehr-project/ehr-extraction-models/datapoint_processor.py...\n",
            "Copying gs://ehr-project/ehr-extraction-models/dataset_stats.ipynb...\n",
            "Copying gs://ehr-project/ehr-extraction-models/hierarchy.py...\n",
            "Copying gs://ehr-project/ehr-extraction-models/interface.py...\n",
            "Copying gs://ehr-project/ehr-extraction-models/model_loader.py...\n",
            "Copying gs://ehr-project/ehr-extraction-models/models/.DS_Store...\n",
            "Copying gs://ehr-project/ehr-extraction-models/models/clinical_bert/README.md...\n",
            "Copying gs://ehr-project/ehr-extraction-models/models/clinical_bert/__pycache__/model.cpython-310.pyc...\n",
            "Copying gs://ehr-project/ehr-extraction-models/models/clinical_bert/__pycache__/model.cpython-312.pyc...\n",
            "Copying gs://ehr-project/ehr-extraction-models/models/clinical_bert/model.py...\n",
            "Copying gs://ehr-project/ehr-extraction-models/models/clusterer/__pycache__/model.cpython-310.pyc...\n",
            "Copying gs://ehr-project/ehr-extraction-models/models/clusterer/__pycache__/model.cpython-312.pyc...\n",
            "Copying gs://ehr-project/ehr-extraction-models/models/clusterer/model.py...\n",
            "Copying gs://ehr-project/ehr-extraction-models/models/code_supervision/__pycache__/model.cpython-310.pyc...\n",
            "Copying gs://ehr-project/ehr-extraction-models/models/code_supervision/__pycache__/model.cpython-312.pyc...\n",
            "Copying gs://ehr-project/ehr-extraction-models/models/code_supervision/__pycache__/postprocessor.cpython-310.pyc...\n",
            "Copying gs://ehr-project/ehr-extraction-models/models/code_supervision/__pycache__/postprocessor.cpython-312.pyc...\n",
            "Copying gs://ehr-project/ehr-extraction-models/models/code_supervision/model.py...\n",
            "Copying gs://ehr-project/ehr-extraction-models/models/code_supervision/postprocessor.py...\n",
            "Copying gs://ehr-project/ehr-extraction-models/models/code_supervision_individual_sentence/__pycache__/model.cpython-310.pyc...\n",
            "Copying gs://ehr-project/ehr-extraction-models/models/code_supervision_individual_sentence/__pycache__/model.cpython-312.pyc...\n",
            "Copying gs://ehr-project/ehr-extraction-models/models/code_supervision_individual_sentence/__pycache__/postprocessor.cpython-310.pyc...\n",
            "Copying gs://ehr-project/ehr-extraction-models/models/code_supervision_individual_sentence/__pycache__/postprocessor.cpython-312.pyc...\n",
            "Copying gs://ehr-project/ehr-extraction-models/models/code_supervision_individual_sentence/model.py...\n",
            "Copying gs://ehr-project/ehr-extraction-models/models/code_supervision_individual_sentence/postprocessor.py...\n",
            "Copying gs://ehr-project/ehr-extraction-models/models/cosine_similarity/__pycache__/model.cpython-310.pyc...\n",
            "Copying gs://ehr-project/ehr-extraction-models/models/cosine_similarity/__pycache__/model.cpython-312.pyc...\n",
            "Copying gs://ehr-project/ehr-extraction-models/models/cosine_similarity/__pycache__/postprocessor.cpython-310.pyc...\n",
            "Copying gs://ehr-project/ehr-extraction-models/models/cosine_similarity/__pycache__/postprocessor.cpython-312.pyc...\n",
            "Copying gs://ehr-project/ehr-extraction-models/models/cosine_similarity/model.py...\n",
            "Copying gs://ehr-project/ehr-extraction-models/models/cosine_similarity/postprocessor.py...\n",
            "Copying gs://ehr-project/ehr-extraction-models/models/distance/__pycache__/model.cpython-310.pyc...\n",
            "Copying gs://ehr-project/ehr-extraction-models/models/distance/__pycache__/model.cpython-312.pyc...\n",
            "Copying gs://ehr-project/ehr-extraction-models/models/distance/model.py...\n",
            "Copying gs://ehr-project/ehr-extraction-models/models/summary_supervision/iteration_info.py...\n",
            "Copying gs://ehr-project/ehr-extraction-models/models/summary_supervision/model.py...\n",
            "Copying gs://ehr-project/ehr-extraction-models/models/tfidf_similarity/__pycache__/model.cpython-310.pyc...\n",
            "Copying gs://ehr-project/ehr-extraction-models/models/tfidf_similarity/__pycache__/model.cpython-312.pyc...\n",
            "Copying gs://ehr-project/ehr-extraction-models/models/tfidf_similarity/model.py...\n",
            "Copying gs://ehr-project/ehr-extraction-models/parameters.py...\n",
            "Copying gs://ehr-project/ehr-extraction-models/processing/.DS_Store...\n",
            "Copying gs://ehr-project/ehr-extraction-models/processing/__pycache__/batcher.cpython-310.pyc...\n",
            "Copying gs://ehr-project/ehr-extraction-models/processing/__pycache__/batcher.cpython-312.pyc...\n",
            "Copying gs://ehr-project/ehr-extraction-models/processing/__pycache__/dataset.cpython-310.pyc...\n",
            "Copying gs://ehr-project/ehr-extraction-models/processing/__pycache__/dataset.cpython-312.pyc...\n",
            "Copying gs://ehr-project/ehr-extraction-models/processing/__pycache__/postprocessor.cpython-310.pyc...\n",
            "Copying gs://ehr-project/ehr-extraction-models/processing/__pycache__/postprocessor.cpython-312.pyc...\n",
            "Copying gs://ehr-project/ehr-extraction-models/processing/__pycache__/tokenizer.cpython-310.pyc...\n",
            "Copying gs://ehr-project/ehr-extraction-models/processing/__pycache__/tokenizer.cpython-312.pyc...\n",
            "Copying gs://ehr-project/ehr-extraction-models/processing/batcher.py...\n",
            "Copying gs://ehr-project/ehr-extraction-models/processing/dataset.py...\n",
            "Copying gs://ehr-project/ehr-extraction-models/processing/postprocessor.py...\n",
            "Copying gs://ehr-project/ehr-extraction-models/processing/tokenizer.py...\n",
            "Copying gs://ehr-project/ehr-extraction-models/requirements.txt...\n",
            "Copying gs://ehr-project/ehr-extraction-models/supervised_ananlysis.ipynb...\n",
            "Copying gs://ehr-project/ehr-extraction-models/test.py...\n",
            "Copying gs://ehr-project/ehr-extraction-models/tracker.py...\n",
            "Copying gs://ehr-project/ehr-extraction-models/train.py...\n",
            "Copying gs://ehr-project/ehr-extraction-models/train_test_models.sh...\n",
            "Copying gs://ehr-project/ehr-extraction-models/unsupervised_ananlysis.ipynb...\n",
            "Copying gs://ehr-project/ehr-extraction-models/utils.py...\n",
            "\\ [103 files][  6.2 MiB/  6.2 MiB]  300.9 KiB/s                                 \n",
            "==> NOTE: You are performing a sequence of gsutil operations that may\n",
            "run significantly faster if you instead use gsutil -m cp ... Please\n",
            "see the -m section under \"gsutil help options\" for further information\n",
            "about when gsutil -m can be advantageous.\n",
            "\n",
            "\n",
            "Operation completed over 103 objects/6.2 MiB.                                    \n"
          ]
        }
      ],
      "source": [
        "!gsutil cp -r gs://ehr-project/ehr-extraction-models /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaS-Z3DTChcc",
        "outputId": "9f9bf17c-6479-4470-b186-ed221e43d941"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://ehr-project/pytt/.git/HEAD...\n",
            "Copying gs://ehr-project/pytt/.git/FETCH_HEAD...\n",
            "/ [0/91 files][    0.0 B/490.8 KiB]   0% Done                                   \r/ [0/91 files][    0.0 B/490.8 KiB]   0% Done                                   \rCopying gs://ehr-project/pytt/.git/config...\n",
            "/ [0/91 files][    0.0 B/490.8 KiB]   0% Done                                   \rCopying gs://ehr-project/pytt/.git/hooks/post-update.sample...\n",
            "/ [0/91 files][    0.0 B/490.8 KiB]   0% Done                                   \rCopying gs://ehr-project/pytt/.git/description...\n",
            "Copying gs://ehr-project/pytt/.git/hooks/fsmonitor-watchman.sample...\n",
            "/ [0/91 files][    0.0 B/490.8 KiB]   0% Done                                   \rCopying gs://ehr-project/pytt/.git/hooks/commit-msg.sample...\n",
            "Copying gs://ehr-project/pytt/.git/hooks/pre-applypatch.sample...\n",
            "Copying gs://ehr-project/pytt/.git/hooks/applypatch-msg.sample...\n",
            "Copying gs://ehr-project/pytt/.git/hooks/pre-commit.sample...\n",
            "Copying gs://ehr-project/pytt/.git/hooks/pre-merge-commit.sample...\n",
            "Copying gs://ehr-project/pytt/.git/hooks/pre-push.sample...\n",
            "Copying gs://ehr-project/pytt/.git/hooks/pre-rebase.sample...\n",
            "Copying gs://ehr-project/pytt/.git/hooks/pre-receive.sample...\n",
            "Copying gs://ehr-project/pytt/.git/hooks/prepare-commit-msg.sample...\n",
            "Copying gs://ehr-project/pytt/.git/hooks/push-to-checkout.sample...\n",
            "Copying gs://ehr-project/pytt/.git/hooks/sendemail-validate.sample...\n",
            "Copying gs://ehr-project/pytt/.git/hooks/update.sample...\n",
            "Copying gs://ehr-project/pytt/.git/index...\n",
            "Copying gs://ehr-project/pytt/.git/info/exclude...\n",
            "Copying gs://ehr-project/pytt/.git/logs/HEAD...\n",
            "Copying gs://ehr-project/pytt/.git/logs/refs/heads/master...\n",
            "Copying gs://ehr-project/pytt/.git/logs/refs/remotes/origin/HEAD...\n",
            "Copying gs://ehr-project/pytt/.git/objects/pack/pack-12721ed6cbccd6e80ec343633f3077cebcd32ddd.idx...\n",
            "Copying gs://ehr-project/pytt/.git/objects/pack/pack-12721ed6cbccd6e80ec343633f3077cebcd32ddd.pack...\n",
            "Copying gs://ehr-project/pytt/.git/objects/pack/pack-12721ed6cbccd6e80ec343633f3077cebcd32ddd.rev...\n",
            "Copying gs://ehr-project/pytt/.git/packed-refs...\n",
            "Copying gs://ehr-project/pytt/.git/refs/heads/master...\n",
            "Copying gs://ehr-project/pytt/.git/refs/remotes/origin/HEAD...\n",
            "Copying gs://ehr-project/pytt/.gitignore...\n",
            "Copying gs://ehr-project/pytt/README.md...\n",
            "Copying gs://ehr-project/pytt/pytt.egg-info/PKG-INFO...\n",
            "Copying gs://ehr-project/pytt/pytt.egg-info/SOURCES.txt...\n",
            "Copying gs://ehr-project/pytt/pytt.egg-info/dependency_links.txt...\n",
            "Copying gs://ehr-project/pytt/pytt.egg-info/top_level.txt...\n",
            "Copying gs://ehr-project/pytt/pytt/__pycache__/distributed.cpython-310.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/__pycache__/distributed.cpython-312.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/__pycache__/email.cpython-310.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/__pycache__/email.cpython-312.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/__pycache__/logger.cpython-310.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/__pycache__/logger.cpython-312.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/__pycache__/progress_bar.cpython-310.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/__pycache__/progress_bar.cpython-312.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/__pycache__/utils.cpython-310.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/__pycache__/utils.cpython-312.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/batching/__pycache__/abstract_batcher.cpython-310.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/batching/__pycache__/abstract_batcher.cpython-312.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/batching/__pycache__/indices_iterator.cpython-310.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/batching/__pycache__/indices_iterator.cpython-312.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/batching/__pycache__/postprocessor.cpython-310.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/batching/__pycache__/postprocessor.cpython-312.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/batching/__pycache__/standard_batch_iterator.cpython-310.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/batching/__pycache__/standard_batch_iterator.cpython-312.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/batching/__pycache__/standard_batcher.cpython-310.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/batching/__pycache__/standard_batcher.cpython-312.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/batching/abstract_batcher.py...\n",
            "Copying gs://ehr-project/pytt/pytt/batching/indices_iterator.py...\n",
            "Copying gs://ehr-project/pytt/pytt/batching/postprocessor.py...\n",
            "Copying gs://ehr-project/pytt/pytt/batching/standard_batch_iterator.py...\n",
            "Copying gs://ehr-project/pytt/pytt/batching/standard_batcher.py...\n",
            "Copying gs://ehr-project/pytt/pytt/distributed.py...\n",
            "Copying gs://ehr-project/pytt/pytt/email.py...\n",
            "Copying gs://ehr-project/pytt/pytt/logger.py...\n",
            "Copying gs://ehr-project/pytt/pytt/nlp/tokenizer.py...\n",
            "Copying gs://ehr-project/pytt/pytt/preprocessing/__pycache__/raw_dataset.cpython-310.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/preprocessing/__pycache__/raw_dataset.cpython-312.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/preprocessing/raw_dataset.py...\n",
            "Copying gs://ehr-project/pytt/pytt/progress_bar.py...\n",
            "Copying gs://ehr-project/pytt/pytt/testing/__pycache__/datapoint_processor.cpython-310.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/testing/__pycache__/datapoint_processor.cpython-312.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/testing/__pycache__/tester.cpython-310.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/testing/__pycache__/tester.cpython-312.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/testing/raw_individual_processor.py...\n",
            "Copying gs://ehr-project/pytt/pytt/testing/datapoint_processor.py...\n",
            "Copying gs://ehr-project/pytt/pytt/testing/tester.py...\n",
            "Copying gs://ehr-project/pytt/pytt/training/__pycache__/iteration_info.cpython-310.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/training/__pycache__/iteration_info.cpython-312.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/training/__pycache__/tracker.cpython-310.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/training/__pycache__/tracker.cpython-312.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/training/__pycache__/trainer.cpython-310.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/training/__pycache__/trainer.cpython-312.pyc...\n",
            "Copying gs://ehr-project/pytt/pytt/training/iteration_info.py...\n",
            "Copying gs://ehr-project/pytt/pytt/training/tracker.py...\n",
            "Copying gs://ehr-project/pytt/pytt/training/trainer.py...\n",
            "Copying gs://ehr-project/pytt/pytt/training/training_controller.py...\n",
            "Copying gs://ehr-project/pytt/pytt/utils.py...\n",
            "Copying gs://ehr-project/pytt/requirements.txt...\n",
            "Copying gs://ehr-project/pytt/setup.py...\n",
            "Copying gs://ehr-project/pytt/summarization/summarization_batcher.py...\n",
            "Copying gs://ehr-project/pytt/summarization/summarization_dataset.py...\n",
            "Copying gs://ehr-project/pytt/tests.py...\n",
            "| [91/91 files][490.8 KiB/490.8 KiB] 100% Done                                  \n",
            "Operation completed over 91 objects/490.8 KiB.                                   \n"
          ]
        }
      ],
      "source": [
        "!gsutil -m cp -r gs://ehr-project/pytt /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_6ugpnc06Kp",
        "outputId": "fe06ae88-5c3b-4204-b0e9-a289297a8625"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://ehr-project/preprocessing-ehr/.git/hooks/applypatch-msg.sample...\n",
            "/ [0/85 files][    0.0 B/ 37.8 MiB]   0% Done                                   \rCopying gs://ehr-project/preprocessing-ehr/.DS_Store...\n",
            "/ [0/85 files][    0.0 B/ 37.8 MiB]   0% Done                                   \rCopying gs://ehr-project/preprocessing-ehr/.git/FETCH_HEAD...\n",
            "/ [0/85 files][    0.0 B/ 37.8 MiB]   0% Done                                   \rCopying gs://ehr-project/preprocessing-ehr/.git/hooks/commit-msg.sample...\n",
            "Copying gs://ehr-project/preprocessing-ehr/.git/HEAD...\n",
            "/ [0/85 files][    0.0 B/ 37.8 MiB]   0% Done                                   \r/ [0/85 files][    0.0 B/ 37.8 MiB]   0% Done                                   \rCopying gs://ehr-project/preprocessing-ehr/.git/hooks/fsmonitor-watchman.sample...\n",
            "Copying gs://ehr-project/preprocessing-ehr/.git/description...\n",
            "/ [0/85 files][    0.0 B/ 37.8 MiB]   0% Done                                   \rCopying gs://ehr-project/preprocessing-ehr/.git/hooks/post-update.sample...\n",
            "/ [0/85 files][    0.0 B/ 37.8 MiB]   0% Done                                   \r/ [0/85 files][    0.0 B/ 37.8 MiB]   0% Done                                   \rCopying gs://ehr-project/preprocessing-ehr/.git/config...\n",
            "Copying gs://ehr-project/preprocessing-ehr/.git/hooks/pre-applypatch.sample...\n",
            "Copying gs://ehr-project/preprocessing-ehr/.git/hooks/pre-commit.sample...\n",
            "Copying gs://ehr-project/preprocessing-ehr/.git/hooks/pre-merge-commit.sample...\n",
            "Copying gs://ehr-project/preprocessing-ehr/.git/hooks/pre-push.sample...\n",
            "Copying gs://ehr-project/preprocessing-ehr/.git/hooks/pre-rebase.sample...\n",
            "Copying gs://ehr-project/preprocessing-ehr/.git/hooks/pre-receive.sample...\n",
            "Copying gs://ehr-project/preprocessing-ehr/.git/hooks/prepare-commit-msg.sample...\n",
            "Copying gs://ehr-project/preprocessing-ehr/.git/hooks/push-to-checkout.sample...\n",
            "Copying gs://ehr-project/preprocessing-ehr/.git/hooks/sendemail-validate.sample...\n",
            "Copying gs://ehr-project/preprocessing-ehr/.git/hooks/update.sample...\n",
            "Copying gs://ehr-project/preprocessing-ehr/.git/index...\n",
            "Copying gs://ehr-project/preprocessing-ehr/.git/info/exclude...\n",
            "Copying gs://ehr-project/preprocessing-ehr/.git/logs/HEAD...\n",
            "Copying gs://ehr-project/preprocessing-ehr/.git/logs/refs/heads/master...\n",
            "Copying gs://ehr-project/preprocessing-ehr/.git/logs/refs/remotes/origin/HEAD...\n",
            "Copying gs://ehr-project/preprocessing-ehr/.git/objects/pack/pack-06bb34275d11ae543c789f68ab2d9646ac3e6738.idx...\n",
            "Copying gs://ehr-project/preprocessing-ehr/.git/objects/pack/pack-06bb34275d11ae543c789f68ab2d9646ac3e6738.pack...\n",
            "Copying gs://ehr-project/preprocessing-ehr/.git/objects/pack/pack-06bb34275d11ae543c789f68ab2d9646ac3e6738.rev...\n",
            "Copying gs://ehr-project/preprocessing-ehr/.git/packed-refs...\n",
            "Copying gs://ehr-project/preprocessing-ehr/.git/refs/heads/master...\n",
            "Copying gs://ehr-project/preprocessing-ehr/.git/refs/remotes/origin/HEAD...\n",
            "Copying gs://ehr-project/preprocessing-ehr/.gitignore...\n",
            "Copying gs://ehr-project/preprocessing-ehr/.vscode/launch.json...\n",
            "Copying gs://ehr-project/preprocessing-ehr/LICENSE...\n",
            "Copying gs://ehr-project/preprocessing-ehr/README.md...\n",
            "Copying gs://ehr-project/preprocessing-ehr/assemble_dataset/__init__.py...\n",
            "Copying gs://ehr-project/preprocessing-ehr/assemble_dataset/__pycache__/dataset.cpython-310.pyc...\n",
            "Copying gs://ehr-project/preprocessing-ehr/assemble_dataset/__pycache__/reports_and_codes_dataset.cpython-310.pyc...\n",
            "Copying gs://ehr-project/preprocessing-ehr/assemble_dataset/__pycache__/utils.cpython-310.pyc...\n",
            "Copying gs://ehr-project/preprocessing-ehr/assemble_dataset/dataset.py...\n",
            "Copying gs://ehr-project/preprocessing-ehr/assemble_dataset/icd_and_readmission_dataset.py...\n",
            "Copying gs://ehr-project/preprocessing-ehr/assemble_dataset/reports_and_codes_aligned_dataset.py...\n",
            "Copying gs://ehr-project/preprocessing-ehr/assemble_dataset/reports_and_codes_dataset.py...\n",
            "Copying gs://ehr-project/preprocessing-ehr/assemble_dataset/reports_and_entities_dataset.py...\n",
            "Copying gs://ehr-project/preprocessing-ehr/assemble_dataset/reports_to_seq_dataset.py...\n",
            "Copying gs://ehr-project/preprocessing-ehr/assemble_dataset/reports_txt_dataset.py...\n",
            "Copying gs://ehr-project/preprocessing-ehr/assemble_dataset/utils.py...\n",
            "Copying gs://ehr-project/preprocessing-ehr/convert_codegraph.py...\n",
            "Copying gs://ehr-project/preprocessing-ehr/convert_pickle.py...\n",
            "Copying gs://ehr-project/preprocessing-ehr/generate_mimic3.py...\n",
            "Copying gs://ehr-project/preprocessing-ehr/generate_not_eevents.py...\n",
            "Copying gs://ehr-project/preprocessing-ehr/icd_codegraph.gexf...\n",
            "Copying gs://ehr-project/preprocessing-ehr/icd_codes/.DS_Store...\n",
            "Copying gs://ehr-project/preprocessing-ehr/icd_codes/README.md...\n",
            "Copying gs://ehr-project/preprocessing-ehr/icd_codes/code_graph.pkl...\n",
            "Copying gs://ehr-project/preprocessing-ehr/icd_codes/code_graph_radiology_expanded.pkl...\n",
            "Copying gs://ehr-project/preprocessing-ehr/icd_codes/code_mapping.pkl...\n",
            "Copying gs://ehr-project/preprocessing-ehr/icd_codes/code_mapping_radiology_expanded.pkl...\n",
            "Copying gs://ehr-project/preprocessing-ehr/icd_codes/diagnosis_codes/icd10.txt...\n",
            "Copying gs://ehr-project/preprocessing-ehr/icd_codes/diagnosis_codes/icd10.txt_processed.csv...\n",
            "Copying gs://ehr-project/preprocessing-ehr/icd_codes/diagnosis_codes/icd10to9.csv...\n",
            "Copying gs://ehr-project/preprocessing-ehr/icd_codes/diagnosis_codes/icd9.txt...\n",
            "Copying gs://ehr-project/preprocessing-ehr/icd_codes/diagnosis_codes/icd9.txt_processed.csv...\n",
            "Copying gs://ehr-project/preprocessing-ehr/icd_codes/diagnosis_codes/icd9to10.csv...\n",
            "Copying gs://ehr-project/preprocessing-ehr/icd_codes/diagnosis_codes/icd9to10.pkl...\n",
            "Copying gs://ehr-project/preprocessing-ehr/icd_codes/process_codes.py...\n",
            "Copying gs://ehr-project/preprocessing-ehr/icd_codes/radiology_codes.py...\n",
            "Copying gs://ehr-project/preprocessing-ehr/icd_codes/radiology_codes_outline.txt...\n",
            "Copying gs://ehr-project/preprocessing-ehr/icd_codes/readmission_correlation.py...\n",
            "Copying gs://ehr-project/preprocessing-ehr/mimic_iii_synthetic_claude/ADMISSIONS.csv...\n",
            "Copying gs://ehr-project/preprocessing-ehr/mimic_iii_synthetic_claude/CHARTEVENTS.csv...\n",
            "Copying gs://ehr-project/preprocessing-ehr/mimic_iii_synthetic_claude/DIAGNOSES_ICD.csv...\n",
            "Copying gs://ehr-project/preprocessing-ehr/mimic_iii_synthetic_claude/D_ICD_DIAGNOSES.csv...\n",
            "Copying gs://ehr-project/preprocessing-ehr/mimic_iii_synthetic_claude/ICUSTAYS.csv...\n",
            "Copying gs://ehr-project/preprocessing-ehr/mimic_iii_synthetic_claude/LABEVENTS.csv...\n",
            "Copying gs://ehr-project/preprocessing-ehr/mimic_iii_synthetic_claude/NOTEEVENTS.csv...\n",
            "Copying gs://ehr-project/preprocessing-ehr/mimic_iii_synthetic_claude/PATIENTS.csv...\n",
            "Copying gs://ehr-project/preprocessing-ehr/mimic_iii_synthetic_claude/PRESCRIPTIONS.csv...\n",
            "Copying gs://ehr-project/preprocessing-ehr/mimic_iii_synthetic_claude/PROCEDURES_ICD.csv...\n",
            "Copying gs://ehr-project/preprocessing-ehr/requirements.txt...\n",
            "Copying gs://ehr-project/preprocessing-ehr/run.py...\n",
            "Copying gs://ehr-project/preprocessing-ehr/synthetic_noteevents.csv...\n",
            "Copying gs://ehr-project/preprocessing-ehr/unify_format/convert_bwh.py...\n",
            "Copying gs://ehr-project/preprocessing-ehr/unify_format/convert_mimic.py...\n",
            "Copying gs://ehr-project/preprocessing-ehr/unify_format/convert_mimic_cxr.py...\n",
            "Copying gs://ehr-project/preprocessing-ehr/utils.py...\n",
            "\\ [85/85 files][ 37.8 MiB/ 37.8 MiB] 100% Done                                  \n",
            "Operation completed over 85 objects/37.8 MiB.                                    \n"
          ]
        }
      ],
      "source": [
        "!gsutil -m cp -r gs://ehr-project/preprocessing-ehr /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e0L_ct_1WSi",
        "outputId": "22b09569-fa37-4055-ea1e-569824fa3eae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ClinicalBERT_checkpoint  mimic_synthetic_data_100-assembled  pytt\n",
            "ehr-extraction-models\t preprocessing-ehr\t\t     sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wK0sQoQF1jpW",
        "outputId": "df642c30-fd27-4572-ca21-2594065f159e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pytt\n"
          ]
        }
      ],
      "source": [
        "%cd /content/pytt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNQuhNix1m6b",
        "outputId": "62c62eeb-75d0-46b6-d507-d72452378840"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mpytt\u001b[0m/           README.md         setup.py        tests.py\n",
            "\u001b[01;34mpytt.egg-info\u001b[0m/  requirements.txt  \u001b[01;34msummarization\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TKKKN3b1w5T",
        "outputId": "d182f6e0-7450-4ec8-8f47-b651c24d2d38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/pytt\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Installing collected packages: pytt\n",
            "  Running setup.py develop for pytt\n",
            "Successfully installed pytt-0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xI3zFlkh11Aj",
        "outputId": "b0821f17-2a38-4428-f80c-b56034630d3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pytt\t       README.md\t setup.py\ttests.py\n",
            "pytt.egg-info  requirements.txt  summarization\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "6Ju_acGG13N3"
      },
      "outputs": [],
      "source": [
        "!cd pytt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gCai_Vc198V",
        "outputId": "e544b977-ef77-4ecb-e5c8-5b23105eb908"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 32\n",
            "-rw-r--r-- 1 root root 1294 Apr 22 13:12 README.md\n",
            "drwxr-xr-x 8 root root 4096 Apr 22 13:12 pytt\n",
            "-rw-r--r-- 1 root root  851 Apr 22 13:12 requirements.txt\n",
            "-rw-r--r-- 1 root root  429 Apr 22 13:12 setup.py\n",
            "drwxr-xr-x 2 root root 4096 Apr 22 13:12 summarization\n",
            "-rw-r--r-- 1 root root 8074 Apr 22 13:12 tests.py\n",
            "drwxr-xr-x 2 root root 4096 Apr 22 13:12 pytt.egg-info\n"
          ]
        }
      ],
      "source": [
        "!ls -lrt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3V_tKf-2fOM",
        "outputId": "eea93eb0-16f1-44a9-c8f5-36a50dc5d4d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvoQSJk32i5P",
        "outputId": "1af743a2-0026-4fd9-e5d1-0adfb7c38050"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ehr-extraction-models\n"
          ]
        }
      ],
      "source": [
        "%cd /content/ehr-extraction-models/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PvRLHDk2nyk",
        "outputId": "15f7e09b-b967-4d98-9b60-e9489e5bdf63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 3016\n",
            "-rw-r--r--  1 root root   11357 Apr 22 13:11 LICENSE\n",
            "-rw-r--r--  1 root root    5475 Apr 22 13:11 README.md\n",
            "-rw-r--r--  1 root root    1532 Apr 22 13:11 calculate_random_best.py\n",
            "-rw-r--r--  1 root root 1557828 Apr 22 13:11 context_vector_analysis.ipynb\n",
            "-rw-r--r--  1 root root     280 Apr 22 13:11 convert_model_weights.py\n",
            "-rw-r--r--  1 root root    1598 Apr 22 13:11 datapoint_processor.py\n",
            "-rw-r--r--  1 root root   17620 Apr 22 13:11 dataset_stats.ipynb\n",
            "-rw-r--r--  1 root root    3213 Apr 22 13:11 hierarchy.py\n",
            "-rw-r--r--  1 root root    5175 Apr 22 13:11 interface.py\n",
            "-rw-r--r--  1 root root   10935 Apr 22 13:11 model_loader.py\n",
            "drwxr-xr-x 10 root root    4096 Apr 22 13:11 models\n",
            "-rw-r--r--  1 root root    1593 Apr 22 13:11 parameters.py\n",
            "drwxr-xr-x  3 root root    4096 Apr 22 13:11 processing\n",
            "-rw-r--r--  1 root root    1003 Apr 22 13:11 requirements.txt\n",
            "-rw-r--r--  1 root root 1035459 Apr 22 13:11 supervised_ananlysis.ipynb\n",
            "-rw-r--r--  1 root root    4921 Apr 22 13:11 test.py\n",
            "-rw-r--r--  1 root root     678 Apr 22 13:11 tracker.py\n",
            "-rw-r--r--  1 root root    9732 Apr 22 13:11 train.py\n",
            "-rw-r--r--  1 root root    5232 Apr 22 13:11 train_test_models.sh\n",
            "-rw-r--r--  1 root root  354102 Apr 22 13:11 unsupervised_ananlysis.ipynb\n",
            "-rw-r--r--  1 root root    5068 Apr 22 13:11 utils.py\n"
          ]
        }
      ],
      "source": [
        "!ls -lrt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUbUzg4c8Cz9",
        "outputId": "14dc0c5d-7f83-48e3-f517-71496d6a1bf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QK_M4jM8NtK",
        "outputId": "a5cddf7a-c309-4242-b666-f4ed00375615"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ehr-extraction-models\n"
          ]
        }
      ],
      "source": [
        "%cd /content/ehr-extraction-models/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3LZMbbu85fC",
        "outputId": "42148ee6-cd60-4d6d-98f5-81cbee3e4676"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "id": "WnegFIF69ryf",
        "outputId": "86825a16-edc2-4b76-fc57-3b4f3163d69c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.0.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (75.2.0)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-79.0.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (0.45.1)\n",
            "Downloading pip-25.0.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-79.0.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: setuptools, pip\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.2.0\n",
            "    Uninstalling setuptools-75.2.0:\n",
            "      Successfully uninstalled setuptools-75.2.0\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pip-25.0.1 setuptools-79.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack"
                ]
              },
              "id": "abb63d06589045639af11400432cc27c"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install --upgrade pip setuptools wheel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1LhmEcHh94gg",
        "outputId": "b75b3671-6536-4e67-893b-c00a93ad1a38"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8J1kiFaw97i0",
        "outputId": "05ccb6ab-a56f-459d-afab-71628e11c581"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ehr-extraction-models\n"
          ]
        }
      ],
      "source": [
        "%cd /content/ehr-extraction-models/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnI2GGKm-Ydu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sxK3Ve4-e3P",
        "outputId": "9e8075bc-473e-40aa-eeb6-41836d33ea94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: blis in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
            "Requirement already satisfied: cymem in /usr/local/lib/python3.11/dist-packages (2.0.11)\n",
            "Requirement already satisfied: grpcio in /usr/local/lib/python3.11/dist-packages (1.71.0)\n",
            "Collecting image\n",
            "  Downloading image-1.5.27-py2.py3-none-any.whl.metadata (566 bytes)\n",
            "Requirement already satisfied: kiwisolver in /usr/local/lib/python3.11/dist-packages (1.4.8)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (0.21.1)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from blis) (2.0.2)\n",
            "Collecting django (from image)\n",
            "  Downloading Django-5.2-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from sacremoses) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from sacremoses) (1.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sacremoses) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers) (0.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.13.2)\n",
            "Collecting asgiref>=3.8.1 (from django->image)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: sqlparse>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from django->image) (0.5.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.1.31)\n",
            "Downloading image-1.5.27-py2.py3-none-any.whl (19 kB)\n",
            "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Django-5.2-py3-none-any.whl (8.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: sacremoses, asgiref, django, image\n",
            "Successfully installed asgiref-3.8.1 django-5.2 image-1.5.27 sacremoses-0.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install --only-binary :all: blis cymem grpcio image kiwisolver Pillow regex sacremoses scikit-learn tokenizers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pkl5Q9y-B-3L",
        "outputId": "779ffca4-01b2-4398-f196-0946a1e92ce6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting grpcio==1.53.0\n",
            "  Downloading grpcio-1.53.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting image==1.5.33\n",
            "  Downloading image-1.5.33.tar.gz (15 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sacremoses==0.0.53\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from image==1.5.33) (11.1.0)\n",
            "Requirement already satisfied: django in /usr/local/lib/python3.11/dist-packages (from image==1.5.33) (5.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from image==1.5.33) (1.17.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacremoses==0.0.53) (2024.11.6)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from sacremoses==0.0.53) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from sacremoses==0.0.53) (1.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sacremoses==0.0.53) (4.67.1)\n",
            "Requirement already satisfied: asgiref>=3.8.1 in /usr/local/lib/python3.11/dist-packages (from django->image==1.5.33) (3.8.1)\n",
            "Requirement already satisfied: sqlparse>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from django->image==1.5.33) (0.5.3)\n",
            "Downloading grpcio-1.53.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: image, sacremoses\n",
            "  Building wheel for image (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for image: filename=image-1.5.33-py2.py3-none-any.whl size=19555 sha256=b9a355c6ab37d34792fe9ba1d16655e3f377ad57b8cab50d09de9df9e662346e\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/40/4f/3c9a8d0f22a1a6f966975a460e5cb509a1e7dc42e2ce5d9a6d\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895307 sha256=0c8c75dbacc16ed2b89d11253c9c793e67d65a9ea94ac29ceead45f337f9a68b\n",
            "  Stored in directory: /root/.cache/pip/wheels/11/75/c6/a82d827a00df823caf211262900d2c024f5b3a775b82b45230\n",
            "Successfully built image sacremoses\n",
            "Installing collected packages: sacremoses, grpcio, image\n",
            "  Attempting uninstall: sacremoses\n",
            "    Found existing installation: sacremoses 0.1.1\n",
            "    Uninstalling sacremoses-0.1.1:\n",
            "      Successfully uninstalled sacremoses-0.1.1\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.71.0\n",
            "    Uninstalling grpcio-1.71.0:\n",
            "      Successfully uninstalled grpcio-1.71.0\n",
            "  Attempting uninstall: image\n",
            "    Found existing installation: image 1.5.27\n",
            "    Uninstalling image-1.5.27:\n",
            "      Successfully uninstalled image-1.5.27\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.71.0 requires grpcio>=1.71.0, but you have grpcio 1.53.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed grpcio-1.53.0 image-1.5.33 sacremoses-0.0.53\n"
          ]
        }
      ],
      "source": [
        "!pip install grpcio==1.53.0 image==1.5.33 sacremoses==0.0.53\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owE_FvLRCqyD",
        "outputId": "9ffbd3ab-fb4d-4929-ba5b-3c5c253af390"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ehr-extraction-models\n"
          ]
        }
      ],
      "source": [
        "%cd /content/ehr-extraction-models/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_-9Je4NDnN6",
        "outputId": "16d7019f-66da-422f-dcb6-ab2ee02c46d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ciqh39kZEbfe"
      },
      "outputs": [],
      "source": [
        "!pip install numpy>=1.25\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iFH9wQCEuDU",
        "outputId": "22d0214b-43ab-4225-bf52-83441b4df72a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Collecting numpy\n",
            "  Downloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (0.5.2)\n",
            "Collecting jax\n",
            "  Downloading jax-0.6.0-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (0.5.1)\n",
            "Collecting jaxlib\n",
            "  Downloading jaxlib-0.6.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting ml_dtypes>=0.5.0 (from jax)\n",
            "  Downloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax) (1.14.1)\n",
            "Downloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m113.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jax-0.6.0-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m104.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxlib-0.6.0-cp311-cp311-manylinux2014_x86_64.whl (87.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.8/87.8 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m131.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, ml_dtypes, jaxlib, jax\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: ml_dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.1\n",
            "    Uninstalling ml-dtypes-0.4.1:\n",
            "      Successfully uninstalled ml-dtypes-0.4.1\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.5.1\n",
            "    Uninstalling jaxlib-0.5.1:\n",
            "      Successfully uninstalled jaxlib-0.5.1\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.5.2\n",
            "    Uninstalling jax-0.5.2:\n",
            "      Successfully uninstalled jax-0.5.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.5 which is incompatible.\n",
            "tensorflow 2.18.0 requires ml-dtypes<0.5.0,>=0.4.0, but you have ml-dtypes 0.5.1 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed jax-0.6.0 jaxlib-0.6.0 ml_dtypes-0.5.1 numpy-2.2.5\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade numpy jax jaxlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8p2h4TBbFRjC",
        "outputId": "644501be-0f60-4d3a-d110-caa2d57a457c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ehr-extraction-models\n"
          ]
        }
      ],
      "source": [
        "%cd /content/ehr-extraction-models/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pi5BMYcnGb34",
        "outputId": "3268b02a-ccd6-4921-c600-a49f9a25f451"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: jax 0.6.0\n",
            "Uninstalling jax-0.6.0:\n",
            "  Successfully uninstalled jax-0.6.0\n",
            "Found existing installation: jaxlib 0.6.0\n",
            "Uninstalling jaxlib-0.6.0:\n",
            "  Successfully uninstalled jaxlib-0.6.0\n",
            "Found existing installation: jax-cuda12-plugin 0.5.1\n",
            "Uninstalling jax-cuda12-plugin-0.5.1:\n",
            "  Successfully uninstalled jax-cuda12-plugin-0.5.1\n",
            "Found existing installation: numpy 2.2.5\n",
            "Uninstalling numpy-2.2.5:\n",
            "  Successfully uninstalled numpy-2.2.5\n"
          ]
        }
      ],
      "source": [
        "pip uninstall -y jax jaxlib jax_cuda12_plugin numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNdvgkH3Gk4O"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xuvY5jnHo1N",
        "outputId": "7f262910-3cc9-4ccf-947b-5f7f713b0a16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in links: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
            "Collecting jax\n",
            "  Using cached jax-0.6.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib==0.4.20+cuda11.cudnn86\n",
            "  Downloading https://storage.googleapis.com/jax-releases/cuda11/jaxlib-0.4.20%2Bcuda11.cudnn86-cp311-cp311-manylinux2014_x86_64.whl (135.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.9/135.9 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from jaxlib==0.4.20+cuda11.cudnn86) (1.14.1)\n",
            "Collecting numpy>=1.22 (from jaxlib==0.4.20+cuda11.cudnn86)\n",
            "  Using cached numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jaxlib==0.4.20+cuda11.cudnn86) (0.5.1)\n",
            "INFO: pip is looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting jax\n",
            "  Downloading jax-0.5.3-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading jax-0.5.2-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading jax-0.5.1-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading jax-0.5.0-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading jax-0.4.38-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading jax-0.4.37-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading jax-0.4.36-py3-none-any.whl.metadata (22 kB)\n",
            "INFO: pip is still looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading jax-0.4.35-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading jax-0.4.34-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading jax-0.4.33-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading jax-0.4.31-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading jax-0.4.30-py3-none-any.whl.metadata (22 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading jax-0.4.29-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.11/dist-packages (from jax) (3.4.0)\n",
            "Downloading jax-0.4.29-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "Installing collected packages: numpy, jaxlib, jax\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chex 0.1.89 requires jaxlib>=0.4.27, but you have jaxlib 0.4.20+cuda11.cudnn86 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.5 which is incompatible.\n",
            "orbax-checkpoint 0.11.12 requires jax>=0.5.0, but you have jax 0.4.29 which is incompatible.\n",
            "optax 0.2.4 requires jaxlib>=0.4.27, but you have jaxlib 0.4.20+cuda11.cudnn86 which is incompatible.\n",
            "flax 0.10.5 requires jax>=0.5.1, but you have jax 0.4.29 which is incompatible.\n",
            "tensorflow 2.18.0 requires ml-dtypes<0.5.0,>=0.4.0, but you have ml-dtypes 0.5.1 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed jax-0.4.29 jaxlib-0.4.20+cuda11.cudnn86 numpy-2.2.5\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade jax jaxlib==0.4.20+cuda11.cudnn86 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tCaxLujGHyBP",
        "outputId": "ae67fb2a-777b-491e-95c0-771547553ff5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.24.4\n",
            "  Downloading numpy-1.24.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting scikit-learn==1.4.2\n",
            "  Downloading scikit_learn-1.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting transformers==4.38.2\n",
            "  Downloading transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n",
            "Collecting torch==2.2.2\n",
            "  Downloading torch-2.2.2-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting spacy==3.7.4\n",
            "  Downloading spacy-3.7.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
            "Requirement already satisfied: pandas==2.2.2 in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting tqdm==4.66.2\n",
            "  Downloading tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting matplotlib==3.8.3\n",
            "  Downloading matplotlib-3.8.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: seaborn==0.13.2 in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Collecting jax==0.4.20\n",
            "  Downloading jax-0.4.20-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting jaxlib==0.4.20\n",
            "  Downloading jaxlib-0.4.20-cp311-cp311-manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.4.2) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.4.2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.4.2) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (0.30.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (2.32.3)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers==4.38.2)\n",
            "  Downloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (0.5.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.2)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.2)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.2)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.2)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.2)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.2)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.2)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.2)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.2)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.2)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.2)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.2.0 (from torch==2.2.2)\n",
            "  Downloading triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (3.0.9)\n",
            "Collecting thinc<8.3.0,>=8.2.2 (from spacy==3.7.4)\n",
            "  Downloading thinc-8.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (2.0.10)\n",
            "Collecting weasel<0.4.0,>=0.1.0 (from spacy==3.7.4)\n",
            "  Downloading weasel-0.3.4-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting typer<0.10.0,>=0.3.0 (from spacy==3.7.4)\n",
            "  Downloading typer-0.9.4-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting smart-open<7.0.0,>=5.2.1 (from spacy==3.7.4)\n",
            "  Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (2.11.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (79.0.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.7.4) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.3) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.3) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.3) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.3) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.3) (3.2.3)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax==0.4.20) (0.5.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.11/dist-packages (from jax==0.4.20) (3.4.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2) (12.5.82)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy==3.7.4) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.4) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.4) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.4) (0.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.38.2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.38.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.38.2) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.38.2) (2025.1.31)\n",
            "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy==3.7.4)\n",
            "  Downloading blis-0.7.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy==3.7.4) (0.1.5)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.11/dist-packages (from typer<0.10.0,>=0.3.0->spacy==3.7.4) (8.1.8)\n",
            "Collecting cloudpathlib<0.17.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy==3.7.4)\n",
            "  Downloading cloudpathlib-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.2.2) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.2.2) (1.3.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy==3.7.4) (1.2.1)\n",
            "Downloading numpy-1.24.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m118.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m114.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.2.2-cp311-cp311-manylinux1_x86_64.whl (755.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.6/755.6 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading spacy-3.7.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
            "Downloading matplotlib-3.8.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jax-0.4.20-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxlib-0.4.20-cp311-cp311-manylinux2014_x86_64.whl (85.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.8/85.8 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m141.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m147.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
            "Downloading thinc-8.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (920 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m920.2/920.2 kB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typer-0.9.4-py3-none-any.whl (45 kB)\n",
            "Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
            "Downloading blis-0.7.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m139.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
            "Installing collected packages: typer, triton, tqdm, smart-open, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, cloudpathlib, nvidia-cusolver-cu12, nvidia-cudnn-cu12, blis, torch, tokenizers, scikit-learn, matplotlib, jaxlib, jax, weasel, transformers, thinc, spacy\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.15.2\n",
            "    Uninstalling typer-0.15.2:\n",
            "      Successfully uninstalled typer-0.15.2\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.67.1\n",
            "    Uninstalling tqdm-4.67.1:\n",
            "      Successfully uninstalled tqdm-4.67.1\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 7.1.0\n",
            "    Uninstalling smart-open-7.1.0:\n",
            "      Successfully uninstalled smart-open-7.1.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: cloudpathlib\n",
            "    Found existing installation: cloudpathlib 0.21.0\n",
            "    Uninstalling cloudpathlib-0.21.0:\n",
            "      Successfully uninstalled cloudpathlib-0.21.0\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 1.3.0\n",
            "    Uninstalling blis-1.3.0:\n",
            "      Successfully uninstalled blis-1.3.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.1\n",
            "    Uninstalling tokenizers-0.21.1:\n",
            "      Successfully uninstalled tokenizers-0.21.1\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.10.0\n",
            "    Uninstalling matplotlib-3.10.0:\n",
            "      Successfully uninstalled matplotlib-3.10.0\n",
            "  Attempting uninstall: weasel\n",
            "    Found existing installation: weasel 0.4.1\n",
            "    Uninstalling weasel-0.4.1:\n",
            "      Successfully uninstalled weasel-0.4.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.51.3\n",
            "    Uninstalling transformers-4.51.3:\n",
            "      Successfully uninstalled transformers-4.51.3\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.3.6\n",
            "    Uninstalling thinc-8.3.6:\n",
            "      Successfully uninstalled thinc-8.3.6\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.8.5\n",
            "    Uninstalling spacy-3.8.5:\n",
            "      Successfully uninstalled spacy-3.8.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chex 0.1.89 requires jax>=0.4.27, but you have jax 0.4.20 which is incompatible.\n",
            "chex 0.1.89 requires jaxlib>=0.4.27, but you have jaxlib 0.4.20 which is incompatible.\n",
            "orbax-checkpoint 0.11.12 requires jax>=0.5.0, but you have jax 0.4.20 which is incompatible.\n",
            "blosc2 3.3.0 requires numpy>=1.26, but you have numpy 1.24.4 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.4 which is incompatible.\n",
            "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.38.2 which is incompatible.\n",
            "pymc 5.21.2 requires numpy>=1.25.0, but you have numpy 1.24.4 which is incompatible.\n",
            "optax 0.2.4 requires jax>=0.4.27, but you have jax 0.4.20 which is incompatible.\n",
            "optax 0.2.4 requires jaxlib>=0.4.27, but you have jaxlib 0.4.20 which is incompatible.\n",
            "flax 0.10.5 requires jax>=0.5.1, but you have jax 0.4.20 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.2.2 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.2.2 which is incompatible.\n",
            "tensorflow 2.18.0 requires ml-dtypes<0.5.0,>=0.4.0, but you have ml-dtypes 0.5.1 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed blis-0.7.11 cloudpathlib-0.16.0 jax-0.4.20 jaxlib-0.4.20 matplotlib-3.8.3 numpy-1.24.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvtx-cu12-12.1.105 scikit-learn-1.4.2 smart-open-6.4.0 spacy-3.7.4 thinc-8.2.5 tokenizers-0.15.2 torch-2.2.2 tqdm-4.66.2 transformers-4.38.2 triton-2.2.0 typer-0.9.4 weasel-0.3.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy"
                ]
              },
              "id": "0166ee3a65b44847ae06904905d9c935"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install numpy==1.24.4 scikit-learn==1.4.2 transformers==4.38.2 \\\n",
        "    torch==2.2.2 spacy==3.7.4 pandas==2.2.2 tqdm==4.66.2 \\\n",
        "    matplotlib==3.8.3 seaborn==0.13.2 jax==0.4.20 jaxlib==0.4.20\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYhnYnkNIu20",
        "outputId": "fa474097-1c80-4345-9fdc-51590a4331b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ehr-extraction-models\n"
          ]
        }
      ],
      "source": [
        "%cd /content/ehr-extraction-models/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_GVm8e0JV4q",
        "outputId": "4978765f-37d6-4287-c8d9-acb27b986f39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ehr-extraction-models\n"
          ]
        }
      ],
      "source": [
        "%cd /content/ehr-extraction-models/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umA_EPXol6JV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivior-r8IbPT",
        "outputId": "45a12613-1ed7-47cc-f700-9485adfa921a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://ehr-project/ClinicalBERT_checkpoint/ClinicalBERT_early_notes_pytorch_checkpoint/bert_config.json...\n",
            "/ [0/9 files][    0.0 B/  1.2 GiB]   0% Done                                    \rCopying gs://ehr-project/ClinicalBERT_checkpoint/ClinicalBERT_discharge_pytorch_checkpoint/.DS_Store...\n",
            "/ [0/9 files][    0.0 B/  1.2 GiB]   0% Done                                    \rCopying gs://ehr-project/ClinicalBERT_checkpoint/ClinicalBERT_discharge_pytorch_checkpoint/bert_config.json...\n",
            "/ [0/9 files][    0.0 B/  1.2 GiB]   0% Done                                    \rCopying gs://ehr-project/ClinicalBERT_checkpoint/.DS_Store...\n",
            "/ [0/9 files][    0.0 B/  1.2 GiB]   0% Done                                    \rCopying gs://ehr-project/ClinicalBERT_checkpoint/ClinicalBERT_discharge_pytorch_checkpoint/pytorch_model.bin...\n",
            "/ [0/9 files][    0.0 B/  1.2 GiB]   0% Done                                    \rCopying gs://ehr-project/ClinicalBERT_checkpoint/ClinicalBERT_early_notes_pytorch_checkpoint/pytorch_model.bin...\n",
            "/ [0/9 files][    0.0 B/  1.2 GiB]   0% Done                                    \rCopying gs://ehr-project/ClinicalBERT_checkpoint/ClinicalBERT_discharge_pytorch_checkpoint/vocab.txt...\n",
            "Copying gs://ehr-project/ClinicalBERT_checkpoint/ClinicalBERT_pretraining_pytorch_checkpoint/bert_config.json...\n",
            "/ [0/9 files][    0.0 B/  1.2 GiB]   0% Done                                    \rCopying gs://ehr-project/ClinicalBERT_checkpoint/ClinicalBERT_pretraining_pytorch_checkpoint/pytorch_model.bin...\n",
            "/ [0/9 files][    0.0 B/  1.2 GiB]   0% Done                                    \r/ [0/9 files][    0.0 B/  1.2 GiB]   0% Done                                    \r==> NOTE: You are downloading one or more large file(s), which would\n",
            "run significantly faster if you enabled sliced object downloads. This\n",
            "feature is enabled by default but requires that compiled crcmod be\n",
            "installed (see \"gsutil help crcmod\").\n",
            "\n",
            "- [9/9 files][  1.2 GiB/  1.2 GiB] 100% Done  71.3 MiB/s ETA 00:00:00           \n",
            "Operation completed over 9 objects/1.2 GiB.                                      \n"
          ]
        }
      ],
      "source": [
        "!gsutil -m cp -r gs://ehr-project/ClinicalBERT_checkpoint /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZib1zL8Keml",
        "outputId": "213c2304-b620-4bd5-e140-57ec2907a822"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ClinicalBERT_checkpoint/ClinicalBERT_pretraining_pytorch_checkpoint\n"
          ]
        }
      ],
      "source": [
        "cd /content/ClinicalBERT_checkpoint/ClinicalBERT_pretraining_pytorch_checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDtTiyZhKn_7",
        "outputId": "6aa3fbdc-6741-4045-aca7-4d51d2f7b1f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ehr-extraction-models\n"
          ]
        }
      ],
      "source": [
        "cd /content/ehr-extraction-models/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GEorPZan55R",
        "outputId": "15413667-a015-4f5d-e78b-1d90e21ad3c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ViWcfNMZn9a_"
      },
      "outputs": [],
      "source": [
        "%mkdir checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Un2bYtI8oDLp",
        "outputId": "f4ec1600-fc91-4463-f354-e79959d123d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ehr-extraction-models\n"
          ]
        }
      ],
      "source": [
        "cd ehr-extraction-models/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Zip all files in /content\n",
        "shutil.make_archive('/content/all_colab_outputs_final', 'zip', '/content')\n",
        "\n",
        "# Download the zip file\n",
        "files.download('/content/all_colab_outputs_final.zip')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "-uJyrKtNwtTz",
        "outputId": "0743fde6-5532-4725-bd49-c65e7098553d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e6c9eba5-f7d1-4079-a463-fd4acff6ed6b\", \"all_colab_outputs_final.zip\", 4120559459)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmIDJF6O29bf",
        "outputId": "f6957bef-5862-4aa8-e06a-90ec8e74ed61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-22 13:27:15.423022: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745328435.449330    7384 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745328435.457205    7384 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-22 13:27:15.481880: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[1, 2, 3]\n",
            "[1, 2]\n",
            "[1, 2, 3]\n",
            "[1, 2]\n",
            "/usr/local/lib/python3.11/dist-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.8.0) was trained with spaCy v3.8.0 and may not be 100% compatible with the current version (3.7.4). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n",
            "  0% 0/21 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 1 of 4\n",
            "  0% 0/21 [05:28<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 2 of 4\n",
            "  0% 0/21 [11:20<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 3 of 4\n",
            "  0% 0/21 [16:37<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 4 of 4\n",
            "epochs_seen: 0 of 1, batches_seen: 1 of 21, samples_seen: 4, batch_size: 4\n",
            "  TRAIN\n",
            "    loss: 0.618248\n",
            "    attention_entropy: 6.249980\n",
            "    traceback_attention_entropy: 8.622754\n",
            "    accuracy: 0.445862\n",
            "    macro_averaged - p: 0.000000, r: 0.000000, f1: 0.000000\n",
            "    micro_averaged - p: 0.000000, r: 0.000000, f1: 0.000000\n",
            "  5% 1/21 [17:27<5:49:12, 1047.64s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 1 of 4\n",
            "  5% 1/21 [22:59<5:49:12, 1047.64s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 2 of 4\n",
            "  5% 1/21 [28:33<5:49:12, 1047.64s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 3 of 4\n",
            "  5% 1/21 [33:40<5:49:12, 1047.64s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 4 of 4\n",
            "epochs_seen: 0 of 1, batches_seen: 2 of 21, samples_seen: 8, batch_size: 4\n",
            "  TRAIN\n",
            "    loss: 0.418706\n",
            "    attention_entropy: 6.873798\n",
            "    traceback_attention_entropy: 9.246999\n",
            "    accuracy: 0.677623\n",
            "    macro_averaged - p: 0.713707, r: 0.860465, f1: 0.763012\n",
            "    micro_averaged - p: 0.666667, r: 1.000000, f1: 0.800000\n",
            " 10% 2/21 [39:11<6:19:27, 1198.31s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 1 of 4\n",
            " 10% 2/21 [41:54<6:19:27, 1198.31s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 2 of 4\n",
            " 10% 2/21 [42:23<6:19:27, 1198.31s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 3 of 4\n",
            " 10% 2/21 [47:41<6:19:27, 1198.31s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 4 of 4\n",
            "epochs_seen: 0 of 1, batches_seen: 3 of 21, samples_seen: 12, batch_size: 4\n",
            "  TRAIN\n",
            "    loss: 0.353913\n",
            "    attention_entropy: 5.557222\n",
            "    traceback_attention_entropy: 7.964096\n",
            "    accuracy: 0.310297\n",
            "    macro_averaged - p: 0.000000, r: 0.000000, f1: 0.000000\n",
            "    micro_averaged - p: 0.000000, r: 0.000000, f1: 0.000000\n",
            " 14% 3/21 [53:19<5:11:30, 1038.34s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 1 of 4\n",
            " 14% 3/21 [55:19<5:11:30, 1038.34s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 2 of 4\n",
            " 14% 3/21 [57:55<5:11:30, 1038.34s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 3 of 4\n",
            " 14% 3/21 [1:03:18<5:11:30, 1038.34s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 4 of 4\n",
            "epochs_seen: 0 of 1, batches_seen: 4 of 21, samples_seen: 16, batch_size: 4\n",
            "  TRAIN\n",
            "    loss: 0.502028\n",
            "    attention_entropy: 6.228491\n",
            "    traceback_attention_entropy: 8.672283\n",
            "    accuracy: 0.413095\n",
            "    macro_averaged - p: 0.000000, r: 0.000000, f1: 0.000000\n",
            "    micro_averaged - p: 0.000000, r: 0.000000, f1: 0.000000\n",
            " 19% 4/21 [1:07:02<4:30:05, 953.28s/it] /usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 1 of 4\n",
            " 19% 4/21 [1:10:58<4:30:05, 953.28s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 2 of 4\n",
            " 19% 4/21 [1:16:23<4:30:05, 953.28s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 3 of 4\n",
            " 19% 4/21 [1:19:25<4:30:05, 953.28s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 4 of 4\n",
            "epochs_seen: 0 of 1, batches_seen: 5 of 21, samples_seen: 20, batch_size: 4\n",
            "  TRAIN\n",
            "    loss: 0.277209\n",
            "    attention_entropy: 6.550868\n",
            "    traceback_attention_entropy: 9.017030\n",
            "    accuracy: 0.295249\n",
            "    macro_averaged - p: 0.000000, r: 0.000000, f1: 0.000000\n",
            "    micro_averaged - p: 0.000000, r: 0.000000, f1: 0.000000\n",
            " 24% 5/21 [1:23:26<4:17:12, 964.54s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 1 of 4\n",
            " 24% 5/21 [1:29:01<4:17:12, 964.54s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 2 of 4\n",
            " 24% 5/21 [1:33:38<4:17:12, 964.54s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 3 of 4\n",
            " 24% 5/21 [1:37:52<4:17:12, 964.54s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 4 of 4\n",
            "epochs_seen: 0 of 1, batches_seen: 6 of 21, samples_seen: 24, batch_size: 4\n",
            "  TRAIN\n",
            "    loss: 0.347317\n",
            "    attention_entropy: 6.786693\n",
            "    traceback_attention_entropy: 9.211548\n",
            "    accuracy: 0.258013\n",
            "    macro_averaged - p: 0.000000, r: 0.000000, f1: 0.000000\n",
            "    micro_averaged - p: 0.000000, r: 0.000000, f1: 0.000000\n",
            " 29% 6/21 [1:43:34<4:21:46, 1047.12s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 1 of 4\n",
            " 29% 6/21 [1:46:04<4:21:46, 1047.12s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 2 of 4\n",
            " 29% 6/21 [1:51:43<4:21:46, 1047.12s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 3 of 4\n",
            " 29% 6/21 [1:57:33<4:21:46, 1047.12s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 4 of 4\n",
            "epochs_seen: 0 of 1, batches_seen: 7 of 21, samples_seen: 28, batch_size: 4\n",
            "  TRAIN\n",
            "    loss: 0.336621\n",
            "    attention_entropy: 6.535788\n",
            "    traceback_attention_entropy: 8.956378\n",
            "    accuracy: 0.321111\n",
            "    macro_averaged - p: 0.000000, r: 0.000000, f1: 0.000000\n",
            "    micro_averaged - p: 0.000000, r: 0.000000, f1: 0.000000\n",
            " 33% 7/21 [2:00:29<4:01:56, 1036.87s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 1 of 4\n",
            " 33% 7/21 [2:01:47<4:01:56, 1036.87s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 2 of 4\n",
            " 33% 7/21 [2:03:27<4:01:56, 1036.87s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 3 of 4\n",
            " 33% 7/21 [2:08:41<4:01:56, 1036.87s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 4 of 4\n",
            "epochs_seen: 0 of 1, batches_seen: 8 of 21, samples_seen: 32, batch_size: 4\n",
            "  TRAIN\n",
            "    loss: 0.887148\n",
            "    attention_entropy: 5.519543\n",
            "    traceback_attention_entropy: 7.914603\n",
            "    accuracy: 0.632279\n",
            "    macro_averaged - p: 0.000000, r: 0.000000, f1: 0.000000\n",
            "    micro_averaged - p: 0.000000, r: 0.000000, f1: 0.000000\n",
            " 38% 8/21 [2:10:02<3:12:38, 889.08s/it] /usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 1 of 4\n",
            " 38% 8/21 [2:11:27<3:12:38, 889.08s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 2 of 4\n",
            " 38% 8/21 [2:14:21<3:12:38, 889.08s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 3 of 4\n",
            " 38% 8/21 [2:15:21<3:12:38, 889.08s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 4 of 4\n",
            "epochs_seen: 0 of 1, batches_seen: 9 of 21, samples_seen: 36, batch_size: 4\n",
            "  TRAIN\n",
            "    loss: 0.581094\n",
            "    attention_entropy: 5.144840\n",
            "    traceback_attention_entropy: 7.560487\n",
            "    accuracy: 0.413782\n",
            "    macro_averaged - p: 0.000000, r: 0.000000, f1: 0.000000\n",
            "    micro_averaged - p: 0.000000, r: 0.000000, f1: 0.000000\n",
            " 43% 9/21 [2:16:24<2:26:06, 730.51s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 1 of 4\n",
            " 43% 9/21 [2:28:55<2:26:06, 730.51s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 2 of 4\n",
            " 43% 9/21 [2:34:32<2:26:06, 730.51s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 3 of 4\n",
            " 43% 9/21 [2:36:14<2:26:06, 730.51s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 4 of 4\n",
            "epochs_seen: 0 of 1, batches_seen: 10 of 21, samples_seen: 40, batch_size: 4\n",
            "  TRAIN\n",
            "    loss: 0.298127\n",
            "    attention_entropy: 6.172631\n",
            "    traceback_attention_entropy: 8.622965\n",
            "    accuracy: 0.257407\n",
            "    macro_averaged - p: 0.000000, r: 0.000000, f1: 0.000000\n",
            "    micro_averaged - p: 0.000000, r: 0.000000, f1: 0.000000\n",
            "  VAL\n",
            "    loss: 0.532785\n",
            "    attention_entropy: 6.372943\n",
            "    traceback_attention_entropy: 8.981379\n",
            "    accuracy: 0.438988\n",
            "    macro_averaged - p: 0.000000, r: 0.000000, f1: 0.000000\n",
            "    micro_averaged - p: 0.000000, r: 0.000000, f1: 0.000000\n",
            "saving checkpoint to /content/checkpoint, batches_seen: 10\n",
            " 48% 10/21 [2:38:21<2:47:07, 911.57s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 1 of 4\n",
            " 48% 10/21 [2:42:06<2:47:07, 911.57s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 2 of 4\n",
            " 48% 10/21 [2:43:01<2:47:07, 911.57s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 3 of 4\n",
            " 48% 10/21 [2:48:49<2:47:07, 911.57s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 4 of 4\n",
            "epochs_seen: 0 of 1, batches_seen: 11 of 21, samples_seen: 44, batch_size: 4\n",
            "  TRAIN\n",
            "    loss: 0.533439\n",
            "    attention_entropy: 6.219429\n",
            "    traceback_attention_entropy: 8.654793\n",
            "    accuracy: 0.380135\n",
            "    macro_averaged - p: 0.000000, r: 0.000000, f1: 0.000000\n",
            "    micro_averaged - p: 0.000000, r: 0.000000, f1: 0.000000\n",
            " 52% 11/21 [2:53:52<2:32:54, 917.46s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 1 of 4\n",
            " 52% 11/21 [2:59:13<2:32:54, 917.46s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 2 of 4\n",
            " 52% 11/21 [3:05:01<2:32:54, 917.46s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 3 of 4\n",
            " 52% 11/21 [3:06:29<2:32:54, 917.46s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 4 of 4\n",
            "epochs_seen: 0 of 1, batches_seen: 12 of 21, samples_seen: 48, batch_size: 4\n",
            "  TRAIN\n",
            "    loss: 0.610677\n",
            "    attention_entropy: 5.728192\n",
            "    traceback_attention_entropy: 8.203316\n",
            "    accuracy: 0.435633\n",
            "    macro_averaged - p: 0.000000, r: 0.000000, f1: 0.000000\n",
            "    micro_averaged - p: 0.000000, r: 0.000000, f1: 0.000000\n",
            " 57% 12/21 [3:07:26<2:12:53, 885.95s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 1 of 4\n",
            " 57% 12/21 [3:12:37<2:12:53, 885.95s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 2 of 4\n",
            " 57% 12/21 [3:18:05<2:12:53, 885.95s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 3 of 4\n",
            " 57% 12/21 [3:23:17<2:12:53, 885.95s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 4 of 4\n",
            "epochs_seen: 0 of 1, batches_seen: 13 of 21, samples_seen: 52, batch_size: 4\n",
            "  TRAIN\n",
            "    loss: 0.828409\n",
            "    attention_entropy: 6.419766\n",
            "    traceback_attention_entropy: 8.842326\n",
            "    accuracy: 0.604431\n",
            "    macro_averaged - p: 0.000000, r: 0.000000, f1: 0.000000\n",
            "    micro_averaged - p: 0.000000, r: 0.000000, f1: 0.000000\n",
            " 62% 13/21 [3:24:25<2:03:30, 926.32s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 1 of 4\n",
            " 62% 13/21 [3:28:15<2:03:30, 926.32s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 2 of 4\n",
            " 62% 13/21 [3:29:07<2:03:30, 926.32s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 3 of 4\n",
            " 62% 13/21 [3:30:04<2:03:30, 926.32s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 4 of 4\n",
            "epochs_seen: 0 of 1, batches_seen: 14 of 21, samples_seen: 56, batch_size: 4\n",
            "  TRAIN\n",
            "    loss: 0.489256\n",
            "    attention_entropy: 4.858833\n",
            "    traceback_attention_entropy: 7.231270\n",
            "    accuracy: 0.359539\n",
            "    macro_averaged - p: 0.000000, r: 0.000000, f1: 0.000000\n",
            "    micro_averaged - p: 0.000000, r: 0.000000, f1: 0.000000\n",
            " 67% 14/21 [3:30:42<1:28:43, 760.45s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 1 of 4\n",
            " 67% 14/21 [3:31:16<1:28:43, 760.45s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 2 of 4\n",
            " 67% 14/21 [3:32:47<1:28:43, 760.45s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 3 of 4\n",
            " 67% 14/21 [3:38:17<1:28:43, 760.45s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 4 of 4\n",
            "epochs_seen: 0 of 1, batches_seen: 15 of 21, samples_seen: 60, batch_size: 4\n",
            "  TRAIN\n",
            "    loss: 0.696042\n",
            "    attention_entropy: 5.186465\n",
            "    traceback_attention_entropy: 7.599349\n",
            "    accuracy: 0.510956\n",
            "    macro_averaged - p: 0.000000, r: 0.000000, f1: 0.000000\n",
            "    micro_averaged - p: 0.000000, r: 0.000000, f1: 0.000000\n",
            " 71% 15/21 [3:39:52<1:09:41, 696.97s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 1 of 4\n",
            " 71% 15/21 [3:42:29<1:09:41, 696.97s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 2 of 4\n",
            " 71% 15/21 [3:47:44<1:09:41, 696.97s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 3 of 4\n",
            " 71% 15/21 [3:49:56<1:09:41, 696.97s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 4 of 4\n",
            "epochs_seen: 0 of 1, batches_seen: 16 of 21, samples_seen: 64, batch_size: 4\n",
            "  TRAIN\n",
            "    loss: 0.428246\n",
            "    attention_entropy: 6.460215\n",
            "    traceback_attention_entropy: 8.917301\n",
            "    accuracy: 0.312128\n",
            "    macro_averaged - p: 0.000000, r: 0.000000, f1: 0.000000\n",
            "    micro_averaged - p: 0.000000, r: 0.000000, f1: 0.000000\n",
            " 76% 16/21 [3:55:20<1:03:53, 766.67s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 1 of 4\n",
            " 76% 16/21 [3:58:22<1:03:53, 766.67s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 2 of 4\n",
            " 76% 16/21 [4:00:14<1:03:53, 766.67s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 3 of 4\n",
            " 76% 16/21 [4:00:49<1:03:53, 766.67s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 4 of 4\n",
            "epochs_seen: 0 of 1, batches_seen: 17 of 21, samples_seen: 68, batch_size: 4\n",
            "  TRAIN\n",
            "    loss: 0.574207\n",
            "    attention_entropy: 5.504996\n",
            "    traceback_attention_entropy: 7.924313\n",
            "    accuracy: 0.427372\n",
            "    macro_averaged - p: 0.000000, r: 0.000000, f1: 0.000000\n",
            "    micro_averaged - p: 0.000000, r: 0.000000, f1: 0.000000\n",
            " 81% 17/21 [4:06:00<48:33, 728.39s/it]  /usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 1 of 4\n",
            " 81% 17/21 [4:08:27<48:33, 728.39s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 2 of 4\n",
            " 81% 17/21 [4:14:11<48:33, 728.39s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 3 of 4\n",
            " 81% 17/21 [4:18:04<48:33, 728.39s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 4 of 4\n",
            "epochs_seen: 0 of 1, batches_seen: 18 of 21, samples_seen: 72, batch_size: 4\n",
            "  TRAIN\n",
            "    loss: 0.908801\n",
            "    attention_entropy: 6.206253\n",
            "    traceback_attention_entropy: 8.677617\n",
            "    accuracy: 0.654480\n",
            "    macro_averaged - p: 0.000000, r: 0.000000, f1: 0.000000\n",
            "    micro_averaged - p: 0.000000, r: 0.000000, f1: 0.000000\n",
            " 86% 18/21 [4:19:44<37:51, 757.23s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 1 of 4\n",
            " 86% 18/21 [4:25:22<37:51, 757.23s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 2 of 4\n",
            " 86% 18/21 [4:29:52<37:51, 757.23s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 3 of 4\n",
            " 86% 18/21 [4:32:10<37:51, 757.23s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 4 of 4\n",
            "epochs_seen: 0 of 1, batches_seen: 19 of 21, samples_seen: 76, batch_size: 4\n",
            "  TRAIN\n",
            "    loss: 0.832972\n",
            "    attention_entropy: 6.521225\n",
            "    traceback_attention_entropy: 8.995869\n",
            "    accuracy: 0.600278\n",
            "    macro_averaged - p: 0.000000, r: 0.000000, f1: 0.000000\n",
            "    micro_averaged - p: 0.000000, r: 0.000000, f1: 0.000000\n",
            " 90% 19/21 [4:36:41<27:50, 835.17s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 1 of 4\n",
            " 90% 19/21 [4:49:17<27:50, 835.17s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 2 of 4\n",
            " 90% 19/21 [4:50:26<27:50, 835.17s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 3 of 4\n",
            " 90% 19/21 [4:53:21<27:50, 835.17s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 4 of 4\n",
            "epochs_seen: 0 of 1, batches_seen: 20 of 21, samples_seen: 80, batch_size: 4\n",
            "  TRAIN\n",
            "    loss: 0.210912\n",
            "    attention_entropy: 5.164036\n",
            "    traceback_attention_entropy: 7.516893\n",
            "    accuracy: 0.169279\n",
            "    macro_averaged - p: 0.000000, r: 0.000000, f1: 0.000000\n",
            "    micro_averaged - p: 0.000000, r: 0.000000, f1: 0.000000\n",
            "  VAL\n",
            "    loss: 0.555318\n",
            "    attention_entropy: 6.117738\n",
            "    traceback_attention_entropy: 8.470588\n",
            "    accuracy: 0.407440\n",
            "    macro_averaged - p: 0.000000, r: 0.000000, f1: 0.000000\n",
            "    micro_averaged - p: 0.000000, r: 0.000000, f1: 0.000000\n",
            "saving checkpoint to /content/checkpoint, batches_seen: 20\n",
            " 95% 20/21 [4:54:01<14:56, 896.58s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "        samples_in_subbatch: 1, sequential_subbatch_num: 1 of 1\n",
            "epochs_seen: 1 of 1, batches_seen: 21 of 21, samples_seen: 81, batch_size: 1\n",
            "  TRAIN\n",
            "    loss: 0.736563\n",
            "    attention_entropy: 6.907754\n",
            "    traceback_attention_entropy: 9.187777\n",
            "    accuracy: 0.531250\n",
            "    macro_averaged - p: 0.000000, r: 0.000000, f1: 0.000000\n",
            "    micro_averaged - p: 0.000000, r: 0.000000, f1: 0.000000\n",
            "saving checkpoint to /content/checkpoint, batches_seen: 21\n",
            "copying to checkpoint number 21\n",
            "continuing\n",
            "100% 21/21 [4:59:43<00:00, 730.33s/it]Traceback (most recent call last):\n",
            "  File \"/content/ehr-extraction-models/train.py\", line 164, in <module>\n",
            "    raise e\n",
            "  File \"/content/ehr-extraction-models/train.py\", line 151, in <module>\n",
            "    main(args.model_type, train_file, hierarchy, counts_file, val_file=val_file,\n",
            "  File \"/content/ehr-extraction-models/train.py\", line 99, in main\n",
            "    trainer.train()\n",
            "  File \"/content/pytt/pytt/training/trainer.py\", line 49, in train\n",
            "    self.iteration(iteration_info, grad_mod=grad_mod)\n",
            "  File \"/content/pytt/pytt/training/trainer.py\", line 64, in iteration\n",
            "    self.tracker.register_iteration(iteration_info, self)\n",
            "  File \"/content/ehr-extraction-models/tracker.py\", line 14, in register_iteration\n",
            "    self.expensive_val_func(iteration_info)\n",
            "TypeError: 'NoneType' object is not callable\n",
            "done copying to checkpoint number 21\n",
            "100% 21/21 [4:59:47<00:00, 856.54s/it]\n"
          ]
        }
      ],
      "source": [
        "!python train.py --data_dir /content/mimic_synthetic_data_100-assembled --save_checkpoint_folder /content/checkpoint code_supervision_only_description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gphVTKhhLJnY"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "4-NmNr773FSr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93aa6706-6890-4bdb-8ab4-d8afd0464a84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-22 22:07:02.869035: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745359622.892946  131670 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745359622.900181  131670 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-22 22:07:02.924017: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[1, 2, 3]\n",
            "[1, 2]\n",
            "[1, 2, 3]\n",
            "[1, 2]\n",
            "/usr/local/lib/python3.11/dist-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.8.0) was trained with spaCy v3.8.0 and may not be 100% compatible with the current version (3.7.4). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n",
            "Loading checkpoint from: /content/checkpoint/model_state.tpkl\n",
            "Skipped loading the following layers due to shape mismatch: ['linear.weight', 'linear.bias']\n",
            "  0% 0/5 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "#########\n",
            "            0         1  ...           6           7\n",
            "0  patient_id   hadm_id  ...  from_table  other_info\n",
            "1       55992       NaN  ...  NOTEEVENTS          {}\n",
            "2       55992  188943.0  ...  NOTEEVENTS          {}\n",
            "3       55992  188943.0  ...  NOTEEVENTS          {}\n",
            "4       55992  188943.0  ...  NOTEEVENTS          {}\n",
            "5       55992  188943.0  ...  NOTEEVENTS          {}\n",
            "6       55992  188943.0  ...  NOTEEVENTS          {}\n",
            "7       55992  188943.0  ...  NOTEEVENTS          {}\n",
            "8       55992  188943.0  ...  NOTEEVENTS          {}\n",
            "9       55992  188943.0  ...  NOTEEVENTS          {}\n",
            "\n",
            "[10 rows x 8 columns]\n",
            "Skipping row 0: invalid literal for int() with base 10: 'patient_id'\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "#########\n",
            "             0         1  ...           6           7\n",
            "0   patient_id   hadm_id  ...  from_table  other_info\n",
            "1        55992       NaN  ...  NOTEEVENTS          {}\n",
            "2        55992  188943.0  ...  NOTEEVENTS          {}\n",
            "3        55992  188943.0  ...  NOTEEVENTS          {}\n",
            "4        55992  188943.0  ...  NOTEEVENTS          {}\n",
            "5        55992  188943.0  ...  NOTEEVENTS          {}\n",
            "6        55992  188943.0  ...  NOTEEVENTS          {}\n",
            "7        55992  188943.0  ...  NOTEEVENTS          {}\n",
            "8        55992  188943.0  ...  NOTEEVENTS          {}\n",
            "9        55992  188943.0  ...  NOTEEVENTS          {}\n",
            "10       55992  188943.0  ...  NOTEEVENTS          {}\n",
            "\n",
            "[11 rows x 8 columns]\n",
            "Skipping row 0: invalid literal for int() with base 10: 'patient_id'\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "#########\n",
            "             0         1  ...           6           7\n",
            "0   patient_id   hadm_id  ...  from_table  other_info\n",
            "1        55992       NaN  ...  NOTEEVENTS          {}\n",
            "2        55992  188943.0  ...  NOTEEVENTS          {}\n",
            "3        55992  188943.0  ...  NOTEEVENTS          {}\n",
            "4        55992  188943.0  ...  NOTEEVENTS          {}\n",
            "5        55992  188943.0  ...  NOTEEVENTS          {}\n",
            "6        55992  188943.0  ...  NOTEEVENTS          {}\n",
            "7        55992  188943.0  ...  NOTEEVENTS          {}\n",
            "8        55992  188943.0  ...  NOTEEVENTS          {}\n",
            "9        55992  188943.0  ...  NOTEEVENTS          {}\n",
            "10       55992  188943.0  ...  NOTEEVENTS          {}\n",
            "11       55992       NaN  ...  NOTEEVENTS          {}\n",
            "12       55992  161385.0  ...  NOTEEVENTS          {}\n",
            "13       55992  161385.0  ...  NOTEEVENTS          {}\n",
            "14       55992  161385.0  ...  NOTEEVENTS          {}\n",
            "15       55992  161385.0  ...  NOTEEVENTS          {}\n",
            "16       55992  161385.0  ...  NOTEEVENTS          {}\n",
            "17       55992  161385.0  ...  NOTEEVENTS          {}\n",
            "18       55992       NaN  ...  NOTEEVENTS          {}\n",
            "19       55992       NaN  ...  NOTEEVENTS          {}\n",
            "20       55992       NaN  ...  NOTEEVENTS          {}\n",
            "21       55992       NaN  ...  NOTEEVENTS          {}\n",
            "22       55992       NaN  ...  NOTEEVENTS          {}\n",
            "23       55992       NaN  ...  NOTEEVENTS          {}\n",
            "24       55992       NaN  ...  NOTEEVENTS          {}\n",
            "25       55992       NaN  ...  NOTEEVENTS          {}\n",
            "26       55992  141696.0  ...  NOTEEVENTS          {}\n",
            "\n",
            "[27 rows x 8 columns]\n",
            "Skipping row 0: invalid literal for int() with base 10: 'patient_id'\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "#########\n",
            "            0         1  ...           6           7\n",
            "0  patient_id   hadm_id  ...  from_table  other_info\n",
            "1       85704  182869.0  ...  NOTEEVENTS          {}\n",
            "2       85704  182869.0  ...  NOTEEVENTS          {}\n",
            "\n",
            "[3 rows x 8 columns]\n",
            "Skipping row 0: invalid literal for int() with base 10: 'patient_id'\n",
            "batches_seen: 1 of 5, samples_seen: 4, batch_size: 4\n",
            "  Running Stats:\n",
            "    loss: 1.381912\n",
            "    attention_entropy: 5.082353\n",
            "    traceback_attention_entropy: 7.659326\n",
            "    accuracy: 0.485412\n",
            "    macro_averaged - p: 0.006426, r: 0.006628, f1: 0.005604\n",
            "    micro_averaged - p: 0.005424, r: 0.435897, f1: 0.010714\n",
            " 20% 1/5 [10:51<43:27, 651.89s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "#########\n",
            "             0         1  ...           6           7\n",
            "0   patient_id   hadm_id  ...  from_table  other_info\n",
            "1        90720       NaN  ...  NOTEEVENTS          {}\n",
            "2        90720       NaN  ...  NOTEEVENTS          {}\n",
            "3        90720       NaN  ...  NOTEEVENTS          {}\n",
            "4        90720       NaN  ...  NOTEEVENTS          {}\n",
            "5        90720       NaN  ...  NOTEEVENTS          {}\n",
            "6        90720  191350.0  ...  NOTEEVENTS          {}\n",
            "7        90720  191350.0  ...  NOTEEVENTS          {}\n",
            "8        90720  191350.0  ...  NOTEEVENTS          {}\n",
            "9        90720  191350.0  ...  NOTEEVENTS          {}\n",
            "10       90720  191350.0  ...  NOTEEVENTS          {}\n",
            "11       90720  191350.0  ...  NOTEEVENTS          {}\n",
            "12       90720  191350.0  ...  NOTEEVENTS          {}\n",
            "13       90720  191350.0  ...  NOTEEVENTS          {}\n",
            "14       90720  191350.0  ...  NOTEEVENTS          {}\n",
            "15       90720  191350.0  ...  NOTEEVENTS          {}\n",
            "16       90720  191350.0  ...  NOTEEVENTS          {}\n",
            "17       90720  191350.0  ...  NOTEEVENTS          {}\n",
            "18       90720  191350.0  ...  NOTEEVENTS          {}\n",
            "19       90720  191350.0  ...  NOTEEVENTS          {}\n",
            "20       90720  191350.0  ...  NOTEEVENTS          {}\n",
            "21       90720  191350.0  ...  NOTEEVENTS          {}\n",
            "22       90720  191350.0  ...  NOTEEVENTS          {}\n",
            "23       90720  191350.0  ...  NOTEEVENTS          {}\n",
            "24       90720  191350.0  ...  NOTEEVENTS          {}\n",
            "25       90720  191350.0  ...  NOTEEVENTS          {}\n",
            "26       90720  191350.0  ...  NOTEEVENTS          {}\n",
            "27       90720  191350.0  ...  NOTEEVENTS          {}\n",
            "28       90720  191350.0  ...  NOTEEVENTS          {}\n",
            "\n",
            "[29 rows x 8 columns]\n",
            "Skipping row 0: invalid literal for int() with base 10: 'patient_id'\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "#########\n",
            "            0         1  ...           6           7\n",
            "0  patient_id   hadm_id  ...  from_table  other_info\n",
            "1       22712  131439.0  ...  NOTEEVENTS          {}\n",
            "2       22712  131439.0  ...  NOTEEVENTS          {}\n",
            "3       22712  131439.0  ...  NOTEEVENTS          {}\n",
            "\n",
            "[4 rows x 8 columns]\n",
            "Skipping row 0: invalid literal for int() with base 10: 'patient_id'\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "#########\n",
            "              0         1  ...           6           7\n",
            "0    patient_id   hadm_id  ...  from_table  other_info\n",
            "1         54811       NaN  ...  NOTEEVENTS          {}\n",
            "2         54811       NaN  ...  NOTEEVENTS          {}\n",
            "3         54811  145790.0  ...  NOTEEVENTS          {}\n",
            "4         54811  145790.0  ...  NOTEEVENTS          {}\n",
            "..          ...       ...  ...         ...         ...\n",
            "291       54811  145790.0  ...  NOTEEVENTS          {}\n",
            "292       54811  145790.0  ...  NOTEEVENTS          {}\n",
            "293       54811  145790.0  ...  NOTEEVENTS          {}\n",
            "294       54811  145790.0  ...  NOTEEVENTS          {}\n",
            "295       54811  145790.0  ...  NOTEEVENTS          {}\n",
            "\n",
            "[296 rows x 8 columns]\n",
            "Skipping row 0: invalid literal for int() with base 10: 'patient_id'\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "#########\n",
            "            0         1  ...           6           7\n",
            "0  patient_id   hadm_id  ...  from_table  other_info\n",
            "1        7118  199855.0  ...  NOTEEVENTS          {}\n",
            "2        7118  199855.0  ...  NOTEEVENTS          {}\n",
            "3        7118  199855.0  ...  NOTEEVENTS          {}\n",
            "4        7118  199855.0  ...  NOTEEVENTS          {}\n",
            "5        7118  199855.0  ...  NOTEEVENTS          {}\n",
            "6        7118  199855.0  ...  NOTEEVENTS          {}\n",
            "7        7118  199855.0  ...  NOTEEVENTS          {}\n",
            "8        7118  199855.0  ...  NOTEEVENTS          {}\n",
            "9        7118  199855.0  ...  NOTEEVENTS          {}\n",
            "\n",
            "[10 rows x 8 columns]\n",
            "Skipping row 0: invalid literal for int() with base 10: 'patient_id'\n",
            "batches_seen: 2 of 5, samples_seen: 8, batch_size: 4\n",
            "  Running Stats:\n",
            "    loss: 1.380518\n",
            "    attention_entropy: 5.437492\n",
            "    traceback_attention_entropy: 7.941112\n",
            "    accuracy: 0.485522\n",
            "    macro_averaged - p: 0.008288, r: 0.008713, f1: 0.007039\n",
            "    micro_averaged - p: 0.006775, r: 0.456989, f1: 0.013352\n",
            " 40% 2/5 [24:52<38:09, 763.13s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "#########\n",
            "             0         1  ...           6           7\n",
            "0   patient_id   hadm_id  ...  from_table  other_info\n",
            "1         7118  199855.0  ...  NOTEEVENTS          {}\n",
            "2         7118  199855.0  ...  NOTEEVENTS          {}\n",
            "3         7118  199855.0  ...  NOTEEVENTS          {}\n",
            "4         7118  199855.0  ...  NOTEEVENTS          {}\n",
            "..         ...       ...  ...         ...         ...\n",
            "76        7118  127146.0  ...  NOTEEVENTS          {}\n",
            "77        7118  127146.0  ...  NOTEEVENTS          {}\n",
            "78        7118  127146.0  ...  NOTEEVENTS          {}\n",
            "79        7118  127146.0  ...  NOTEEVENTS          {}\n",
            "80        7118  127146.0  ...  NOTEEVENTS          {}\n",
            "\n",
            "[81 rows x 8 columns]\n",
            "Skipping row 0: invalid literal for int() with base 10: 'patient_id'\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "#########\n",
            "            0         1  ...           6           7\n",
            "0  patient_id   hadm_id  ...  from_table  other_info\n",
            "1        2710  152429.0  ...  NOTEEVENTS          {}\n",
            "2        2710  152429.0  ...  NOTEEVENTS          {}\n",
            "3        2710  152429.0  ...  NOTEEVENTS          {}\n",
            "4        2710  152429.0  ...  NOTEEVENTS          {}\n",
            "5        2710  152429.0  ...  NOTEEVENTS          {}\n",
            "6        2710  152429.0  ...  NOTEEVENTS          {}\n",
            "7        2710  152429.0  ...  NOTEEVENTS          {}\n",
            "8        2710  152429.0  ...  NOTEEVENTS          {}\n",
            "9        2710  152429.0  ...  NOTEEVENTS          {}\n",
            "\n",
            "[10 rows x 8 columns]\n",
            "Skipping row 0: invalid literal for int() with base 10: 'patient_id'\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "#########\n",
            "              0         1  ...           6           7\n",
            "0    patient_id   hadm_id  ...  from_table  other_info\n",
            "1          6329  180506.0  ...  NOTEEVENTS          {}\n",
            "2          6329  180506.0  ...  NOTEEVENTS          {}\n",
            "3          6329  180506.0  ...  NOTEEVENTS          {}\n",
            "4          6329  180506.0  ...  NOTEEVENTS          {}\n",
            "..          ...       ...  ...         ...         ...\n",
            "187        6329  180506.0  ...  NOTEEVENTS          {}\n",
            "188        6329  180506.0  ...  NOTEEVENTS          {}\n",
            "189        6329  180506.0  ...  NOTEEVENTS          {}\n",
            "190        6329  180506.0  ...  NOTEEVENTS          {}\n",
            "191        6329  180506.0  ...  NOTEEVENTS          {}\n",
            "\n",
            "[192 rows x 8 columns]\n",
            "Skipping row 0: invalid literal for int() with base 10: 'patient_id'\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "#########\n",
            "             0         1  ...           6           7\n",
            "0   patient_id   hadm_id  ...  from_table  other_info\n",
            "1        31564  105414.0  ...  NOTEEVENTS          {}\n",
            "2        31564  105414.0  ...  NOTEEVENTS          {}\n",
            "3        31564  105414.0  ...  NOTEEVENTS          {}\n",
            "4        31564  105414.0  ...  NOTEEVENTS          {}\n",
            "5        31564  105414.0  ...  NOTEEVENTS          {}\n",
            "6        31564  105414.0  ...  NOTEEVENTS          {}\n",
            "7        31564  105414.0  ...  NOTEEVENTS          {}\n",
            "8        31564  105414.0  ...  NOTEEVENTS          {}\n",
            "9        31564  105414.0  ...  NOTEEVENTS          {}\n",
            "10       31564  105414.0  ...  NOTEEVENTS          {}\n",
            "11       31564  105414.0  ...  NOTEEVENTS          {}\n",
            "12       31564  105414.0  ...  NOTEEVENTS          {}\n",
            "13       31564  105414.0  ...  NOTEEVENTS          {}\n",
            "14       31564  105414.0  ...  NOTEEVENTS          {}\n",
            "15       31564  105414.0  ...  NOTEEVENTS          {}\n",
            "16       31564  105414.0  ...  NOTEEVENTS          {}\n",
            "17       31564  105414.0  ...  NOTEEVENTS          {}\n",
            "18       31564  105414.0  ...  NOTEEVENTS          {}\n",
            "19       31564  105414.0  ...  NOTEEVENTS          {}\n",
            "20       31564  105414.0  ...  NOTEEVENTS          {}\n",
            "21       31564  105414.0  ...  NOTEEVENTS          {}\n",
            "\n",
            "[22 rows x 8 columns]\n",
            "Skipping row 0: invalid literal for int() with base 10: 'patient_id'\n",
            "batches_seen: 3 of 5, samples_seen: 12, batch_size: 4\n",
            "  Running Stats:\n",
            "    loss: 1.380814\n",
            "    attention_entropy: 5.791170\n",
            "    traceback_attention_entropy: 8.253887\n",
            "    accuracy: 0.485476\n",
            "    macro_averaged - p: 0.007192, r: 0.009430, f1: 0.006493\n",
            "    micro_averaged - p: 0.005526, r: 0.446352, f1: 0.010916\n",
            " 60% 3/5 [40:38<28:12, 846.33s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "#########\n",
            "            0         1  ...           6           7\n",
            "0  patient_id   hadm_id  ...  from_table  other_info\n",
            "1       80970  113253.0  ...  NOTEEVENTS          {}\n",
            "2       80970  113253.0  ...  NOTEEVENTS          {}\n",
            "3       80970  113253.0  ...  NOTEEVENTS          {}\n",
            "4       80970  113253.0  ...  NOTEEVENTS          {}\n",
            "5       80970  113253.0  ...  NOTEEVENTS          {}\n",
            "\n",
            "[6 rows x 8 columns]\n",
            "Skipping row 0: invalid literal for int() with base 10: 'patient_id'\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "#########\n",
            "             0         1  ...           6           7\n",
            "0   patient_id   hadm_id  ...  from_table  other_info\n",
            "1        19493       NaN  ...  NOTEEVENTS          {}\n",
            "2        19493       NaN  ...  NOTEEVENTS          {}\n",
            "3        19493  170699.0  ...  NOTEEVENTS          {}\n",
            "4        19493  170699.0  ...  NOTEEVENTS          {}\n",
            "..         ...       ...  ...         ...         ...\n",
            "63       19493  170699.0  ...  NOTEEVENTS          {}\n",
            "64       19493  170699.0  ...  NOTEEVENTS          {}\n",
            "65       19493  170699.0  ...  NOTEEVENTS          {}\n",
            "66       19493  170699.0  ...  NOTEEVENTS          {}\n",
            "67       19493  170699.0  ...  NOTEEVENTS          {}\n",
            "\n",
            "[68 rows x 8 columns]\n",
            "Skipping row 0: invalid literal for int() with base 10: 'patient_id'\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "#########\n",
            "             0         1  ...           6           7\n",
            "0   patient_id   hadm_id  ...  from_table  other_info\n",
            "1        19493       NaN  ...  NOTEEVENTS          {}\n",
            "2        19493       NaN  ...  NOTEEVENTS          {}\n",
            "3        19493  170699.0  ...  NOTEEVENTS          {}\n",
            "4        19493  170699.0  ...  NOTEEVENTS          {}\n",
            "..         ...       ...  ...         ...         ...\n",
            "67       19493  170699.0  ...  NOTEEVENTS          {}\n",
            "68       19493  170699.0  ...  NOTEEVENTS          {}\n",
            "69       19493  158528.0  ...  NOTEEVENTS          {}\n",
            "70       19493  158528.0  ...  NOTEEVENTS          {}\n",
            "71       19493  158528.0  ...  NOTEEVENTS          {}\n",
            "\n",
            "[72 rows x 8 columns]\n",
            "Skipping row 0: invalid literal for int() with base 10: 'patient_id'\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "#########\n",
            "             0         1  ...           6           7\n",
            "0   patient_id   hadm_id  ...  from_table  other_info\n",
            "1        19493       NaN  ...  NOTEEVENTS          {}\n",
            "2        19493       NaN  ...  NOTEEVENTS          {}\n",
            "3        19493  170699.0  ...  NOTEEVENTS          {}\n",
            "4        19493  170699.0  ...  NOTEEVENTS          {}\n",
            "..         ...       ...  ...         ...         ...\n",
            "68       19493  170699.0  ...  NOTEEVENTS          {}\n",
            "69       19493  158528.0  ...  NOTEEVENTS          {}\n",
            "70       19493  158528.0  ...  NOTEEVENTS          {}\n",
            "71       19493  158528.0  ...  NOTEEVENTS          {}\n",
            "72       19493  158528.0  ...  NOTEEVENTS          {}\n",
            "\n",
            "[73 rows x 8 columns]\n",
            "Skipping row 0: invalid literal for int() with base 10: 'patient_id'\n",
            "batches_seen: 4 of 5, samples_seen: 16, batch_size: 4\n",
            "  Running Stats:\n",
            "    loss: 1.380056\n",
            "    attention_entropy: 5.921869\n",
            "    traceback_attention_entropy: 8.344338\n",
            "    accuracy: 0.485442\n",
            "    macro_averaged - p: 0.006759, r: 0.009634, f1: 0.006210\n",
            "    micro_averaged - p: 0.005140, r: 0.444828, f1: 0.010162\n",
            " 80% 4/5 [56:26<14:46, 886.75s/it]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "#########\n",
            "              0         1  ...           6           7\n",
            "0    patient_id   hadm_id  ...  from_table  other_info\n",
            "1         19493       NaN  ...  NOTEEVENTS          {}\n",
            "2         19493       NaN  ...  NOTEEVENTS          {}\n",
            "3         19493  170699.0  ...  NOTEEVENTS          {}\n",
            "4         19493  170699.0  ...  NOTEEVENTS          {}\n",
            "..          ...       ...  ...         ...         ...\n",
            "98        19493  155882.0  ...  NOTEEVENTS          {}\n",
            "99        19493       NaN  ...  NOTEEVENTS          {}\n",
            "100       19493       NaN  ...  NOTEEVENTS          {}\n",
            "101       19493       NaN  ...  NOTEEVENTS          {}\n",
            "102       19493  163901.0  ...  NOTEEVENTS          {}\n",
            "\n",
            "[103 rows x 8 columns]\n",
            "Skipping row 0: invalid literal for int() with base 10: 'patient_id'\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "#########\n",
            "             0         1  ...           6           7\n",
            "0   patient_id   hadm_id  ...  from_table  other_info\n",
            "1        54739  113210.0  ...  NOTEEVENTS          {}\n",
            "2        54739  113210.0  ...  NOTEEVENTS          {}\n",
            "3        54739  113210.0  ...  NOTEEVENTS          {}\n",
            "4        54739  113210.0  ...  NOTEEVENTS          {}\n",
            "5        54739  113210.0  ...  NOTEEVENTS          {}\n",
            "6        54739  113210.0  ...  NOTEEVENTS          {}\n",
            "7        54739  113210.0  ...  NOTEEVENTS          {}\n",
            "8        54739  113210.0  ...  NOTEEVENTS          {}\n",
            "9        54739  113210.0  ...  NOTEEVENTS          {}\n",
            "10       54739  113210.0  ...  NOTEEVENTS          {}\n",
            "11       54739  113210.0  ...  NOTEEVENTS          {}\n",
            "12       54739  113210.0  ...  NOTEEVENTS          {}\n",
            "13       54739  113210.0  ...  NOTEEVENTS          {}\n",
            "14       54739  113210.0  ...  NOTEEVENTS          {}\n",
            "15       54739  113210.0  ...  NOTEEVENTS          {}\n",
            "16       54739  113210.0  ...  NOTEEVENTS          {}\n",
            "17       54739  113210.0  ...  NOTEEVENTS          {}\n",
            "18       54739  113210.0  ...  NOTEEVENTS          {}\n",
            "19       54739  113210.0  ...  NOTEEVENTS          {}\n",
            "20       54739  113210.0  ...  NOTEEVENTS          {}\n",
            "21       54739  113210.0  ...  NOTEEVENTS          {}\n",
            "22       54739  113210.0  ...  NOTEEVENTS          {}\n",
            "23       54739  113210.0  ...  NOTEEVENTS          {}\n",
            "24       54739  113210.0  ...  NOTEEVENTS          {}\n",
            "25       54739  113210.0  ...  NOTEEVENTS          {}\n",
            "26       54739  113210.0  ...  NOTEEVENTS          {}\n",
            "27       54739  113210.0  ...  NOTEEVENTS          {}\n",
            "28       54739  113210.0  ...  NOTEEVENTS          {}\n",
            "29       54739  113210.0  ...  NOTEEVENTS          {}\n",
            "30       54739  113210.0  ...  NOTEEVENTS          {}\n",
            "31       54739  113210.0  ...  NOTEEVENTS          {}\n",
            "32       54739  113210.0  ...  NOTEEVENTS          {}\n",
            "33       54739  113210.0  ...  NOTEEVENTS          {}\n",
            "34       54739  113210.0  ...  NOTEEVENTS          {}\n",
            "35       54739  113210.0  ...  NOTEEVENTS          {}\n",
            "36       54739  113210.0  ...  NOTEEVENTS          {}\n",
            "37       54739  113210.0  ...  NOTEEVENTS          {}\n",
            "38       54739  113210.0  ...  NOTEEVENTS          {}\n",
            "39       54739  113210.0  ...  NOTEEVENTS          {}\n",
            "40       54739  113210.0  ...  NOTEEVENTS          {}\n",
            "41       54739  113210.0  ...  NOTEEVENTS          {}\n",
            "42       54739  113210.0  ...  NOTEEVENTS          {}\n",
            "43       54739  113210.0  ...  NOTEEVENTS          {}\n",
            "44       54739  113210.0  ...  NOTEEVENTS          {}\n",
            "45       54739  113210.0  ...  NOTEEVENTS          {}\n",
            "46       54739  113210.0  ...  NOTEEVENTS          {}\n",
            "47       54739  113210.0  ...  NOTEEVENTS          {}\n",
            "48       54739  113210.0  ...  NOTEEVENTS          {}\n",
            "49       54739  113210.0  ...  NOTEEVENTS          {}\n",
            "50       54739  113210.0  ...  NOTEEVENTS          {}\n",
            "\n",
            "[51 rows x 8 columns]\n",
            "Skipping row 0: invalid literal for int() with base 10: 'patient_id'\n",
            "batches_seen: 5 of 5, samples_seen: 18, batch_size: 2\n",
            "  Running Stats:\n",
            "    loss: 1.379663\n",
            "    attention_entropy: 6.031412\n",
            "    traceback_attention_entropy: 8.476557\n",
            "    accuracy: 0.485448\n",
            "    macro_averaged - p: 0.006564, r: 0.009719, f1: 0.006006\n",
            "    micro_averaged - p: 0.004746, r: 0.440789, f1: 0.009391\n",
            "100% 5/5 [1:05:42<00:00, 788.41s/it]\n"
          ]
        }
      ],
      "source": [
        "!python test.py code_supervision_only_description /content/checkpoint --results_folder /content/checkpoint/results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzF3GoOel7aV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}